{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00aab400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit.Chem import rdchem, AllChem\n",
    "\n",
    "# 选择设备：GPU如果可用，否则使用CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f87f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据预处理: 从SMILES生成分子图和特征\n",
    "class FeaturizationParameters:\n",
    "    MAX_ATOMIC_NUM = 118  # 最大原子序号\n",
    "\n",
    "    ATOM_FEATURES = {\n",
    "        'atomic_num': list(range(1, MAX_ATOMIC_NUM + 1)),  # 原子序号（1-118）\n",
    "        'degree': [0, 1, 2, 3, 4, 5],  # 原子的度\n",
    "        'formal_charge': [-1, 0, 1],  # 原子的形式电荷\n",
    "        'chiral_tag': [0, 1, 2, 3],  # 手性信息\n",
    "        'num_Hs': [0, 1, 2, 3],  # 连接的H原子数\n",
    "        'hybridization': [\n",
    "            rdchem.HybridizationType.SP,\n",
    "            rdchem.HybridizationType.SP2,\n",
    "            rdchem.HybridizationType.SP3,\n",
    "            rdchem.HybridizationType.SP3D,\n",
    "            rdchem.HybridizationType.SP3D2\n",
    "        ],\n",
    "        'explicit_valence': [0, 1, 2, 3, 4, 5, 6],  # 显式化合价\n",
    "        'implicit_valence': [0, 1, 2, 3],  # 隐式化合价\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # 一个通用的函数用于独热编码\n",
    "    def one_hot_encoding(self, value, feature_list):\n",
    "        return [1 if x == value else 0 for x in feature_list]\n",
    "    \n",
    "def smiles_to_graph(smiles, featurization_params):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "    \n",
    "    # 获取相关的原子特征并进行独热编码\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        # 使用 FeaturizationParameters 类中的 `one_hot_encoding` 方法进行独热编码\n",
    "        features = []\n",
    "        \n",
    "        # 对每个特征进行独热编码\n",
    "        features += featurization_params.one_hot_encoding(atom.GetAtomicNum(), featurization_params.ATOM_FEATURES['atomic_num'])  # 原子序数\n",
    "        features += featurization_params.one_hot_encoding(atom.GetFormalCharge(), featurization_params.ATOM_FEATURES['formal_charge'])  # 电荷\n",
    "        features += featurization_params.one_hot_encoding(atom.GetNumRadicalElectrons(), [0, 1, 2, 3])  # 自由基电子数（假设只有0到3个）\n",
    "        features.append(1 if atom.GetIsAromatic() else 0)  # 是否芳香环\n",
    "        features += featurization_params.one_hot_encoding(atom.GetDegree(), featurization_params.ATOM_FEATURES['degree'])  # 度数\n",
    "        features += featurization_params.one_hot_encoding(atom.GetExplicitValence(), featurization_params.ATOM_FEATURES['explicit_valence'])  # 显式化合价\n",
    "        features += featurization_params.one_hot_encoding(atom.GetImplicitValence(), featurization_params.ATOM_FEATURES['implicit_valence'])  # 隐式化合价\n",
    "        features += featurization_params.one_hot_encoding(atom.GetHybridization(), featurization_params.ATOM_FEATURES['hybridization'])  # 杂化类型\n",
    "        features.append(atom.GetTotalNumHs())  # 连接的氢原子数（不进行独热编码，可以直接使用数值）\n",
    "        \n",
    "        atom_features.append(features)\n",
    "\n",
    "    # 获取键特征\n",
    "    bonds = []\n",
    "    bond_types = []\n",
    "    for bond in mol.GetBonds():\n",
    "        bond_types.append(bond.GetBondTypeAsDouble())\n",
    "        bonds.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    bonds = np.array(bonds, dtype=np.int64)  # 使用np.int64代替np.long\n",
    "    bond_types = np.array(bond_types, dtype=np.float32)\n",
    "\n",
    "    # 将图数据结构化\n",
    "    data = Data(\n",
    "        x=torch.tensor(atom_features, dtype=torch.float32),  # 确保x是浮点型\n",
    "        edge_index=torch.tensor(bonds, dtype=torch.long).t().contiguous(),\n",
    "        edge_attr=torch.tensor(bond_types, dtype=torch.float32)\n",
    "    )\n",
    "    return data, mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29d3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 模型定义: 图神经网络 + MLP\n",
    "class GNN_MLP_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_mol_features, dropout_prob=0.5):\n",
    "        super(GNN_MLP_Model, self).__init__()\n",
    "\n",
    "        # 图卷积层\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)  \n",
    "        \n",
    "        # MLP部分\n",
    "        self.fc1 = nn.Linear(hidden_dim + num_mol_features, hidden_dim)  # 拼接后的维度\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)  # 新增的全连接层\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  # 输出层\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, data, mol_features):\n",
    "        # 获取图的数据\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        # 通过图卷积层并应用 ReLU 激活函数\n",
    "        x = torch.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = torch.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = torch.relu(self.conv3(x, edge_index, edge_attr))  # 第四层卷积\n",
    "        \n",
    "        # 图的全局池化：使用平均池化\n",
    "        x = torch.mean(x, dim=0)  # 对节点特征进行池化\n",
    "        \n",
    "        # 确保分子特征的形状正确\n",
    "        mol_features = mol_features.view(-1, mol_features.size(-1))  # 确保形状正确\n",
    "        \n",
    "        # 拼接图特征和分子特征\n",
    "        combined_features = torch.cat([x.unsqueeze(0), mol_features], dim=-1)  # 拼接操作\n",
    "        \n",
    "\n",
    "        # 通过全连接层并应用 ReLU 激活函数\n",
    "        x = torch.relu(self.fc1(combined_features))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # 输出层\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b37297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 训练过程\n",
    "def train_kfold(model, data, mol_features, targets, epochs=100, lr=1e-3, num_splits=10):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Metrics storage for each fold\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    # Data for saving predictions and actual values\n",
    "    predictions_and_actuals = []\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     mol_features_scaled = scaler.fit_transform(mol_features.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Training and evaluating each fold\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(data)):\n",
    "        print(f\"\\nTraining Fold {fold + 1}/{num_splits}...\")\n",
    "\n",
    "        # Split data\n",
    "        train_data = [data[i] for i in train_index]\n",
    "        val_data = [data[i] for i in val_index]\n",
    "        mol_features_train = mol_features[train_index].clone().detach().to(device)\n",
    "        mol_features_val = mol_features[val_index].clone().detach().to(device)\n",
    "        targets_train = targets[train_index].clone().detach().to(device)\n",
    "        targets_val = targets[val_index].clone().detach().to(device)\n",
    "\n",
    "        # Train the model\n",
    "        model.train()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-3)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(len(train_data)):\n",
    "                data_ = train_data[i].to(device)\n",
    "                mol_feature = mol_features_train[i].to(device)\n",
    "                target = targets_train[i]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data_, mol_feature)\n",
    "                loss = loss_fn(output, target)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation loss\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(val_data)):\n",
    "                    data_ = val_data[i].to(device)\n",
    "                    mol_feature = mol_features_val[i].to(device)\n",
    "                    target = targets_val[i]\n",
    "\n",
    "                    output = model(data_, mol_feature)\n",
    "                    loss = loss_fn(output, target)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {total_loss / len(train_data):.4f}, Validation Loss: {val_loss / len(val_data):.4f}\")\n",
    "\n",
    "            # Save the best model for this fold\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"best_model_fold_{fold + 1}.pth\")\n",
    "                print('best model find! loss:',val_loss, best_val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Load the best model and evaluate\n",
    "        model.load_state_dict(torch.load(f\"best_model_fold_{fold + 1}.pth\"))\n",
    "\n",
    "        # Calculate metrics for validation set\n",
    "        rmse, mae, r2 = calculate_metrics(val_data, mol_features_val, targets_val)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        r2_list.append(r2)\n",
    "\n",
    "        print(f\"Fold {fold + 1} -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Summary statistics\n",
    "    rmse_mean, rmse_std = np.mean(rmse_list), np.std(rmse_list)\n",
    "    mae_mean, mae_std = np.mean(mae_list), np.std(mae_list)\n",
    "    r2_mean, r2_std = np.mean(r2_list), np.std(r2_list)\n",
    "\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    print(f\"RMSE Mean: {rmse_mean:.4f}, Std: {rmse_std:.4f}\")\n",
    "    print(f\"MAE Mean: {mae_mean:.4f}, Std: {mae_std:.4f}\")\n",
    "    print(f\"R² Mean: {r2_mean:.4f}, Std: {r2_std:.4f}\")\n",
    "\n",
    "    return rmse_mean, mae_mean, r2_mean, rmse_std, mae_std, r2_std\n",
    "\n",
    "\n",
    "def calculate_metrics(data, mol_features, targets):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data_ = data[i].to(device)\n",
    "            mol_feature = mol_features[i].to(device)\n",
    "            target = targets[i]\n",
    "\n",
    "            output = model(data_, mol_feature)\n",
    "            predictions.append(output.item())\n",
    "            true_values.append(target.item())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    true_values = np.array(true_values)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "def get_predictions(model, data, mol_features, targets):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data_ = data[i].to(device)\n",
    "            mol_feature = mol_features[i].to(device)\n",
    "            target = targets[i]\n",
    "\n",
    "            output = model(data_, mol_feature)\n",
    "            predictions.append(output.item())\n",
    "            actuals.append(target.item())\n",
    "\n",
    "    return np.array(predictions), np.array(actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252629b",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# 函数加载完毕，处理数据(主函数部分)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "021e157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "data = pd.read_csv(r\"input_zz.csv\")  # 替换为你的数据文件路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bad30f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"filter_dev.csv\")  # 替换为你的数据文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "699170a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.训练集与测试集随机划分:完全随机划分\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7279077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机旋转分子对训练数据进行数据增强\n",
    "def randomize_mol(mol):\n",
    "    try:\n",
    "        ans = list(range(mol.GetNumAtoms()))\n",
    "        np.random.shuffle(ans)\n",
    "        nm = Chem.RenumberAtoms(mol, ans)\n",
    "        return nm\n",
    "    except:\n",
    "        return float('nan')\n",
    "\n",
    "def dataAug(df, epoch):\n",
    "    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n",
    "    dfold = df.copy()\n",
    "    for i in range (epoch-1):\n",
    "        dftmp = dfold.copy()\n",
    "        dftmp['mol'] = dftmp['mol'].apply(randomize_mol)\n",
    "        dftmp['SMILES'] = dftmp['mol'].apply(Chem.MolToSmiles)\n",
    "        df = pd.concat([df, dftmp])\n",
    "    df = df.drop('mol', axis=1)\n",
    "    return df\n",
    "\n",
    "train_data = dataAug(train_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0401ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = train_data['SMILES'].tolist()\n",
    "train_targets = train_data['IE'].values\n",
    "mol_features = train_data.drop(columns=['SMILES', 'IE']).values\n",
    "\n",
    "# 分子图特征提取\n",
    "featurization_params = FeaturizationParameters()\n",
    "data_list = []\n",
    "for smiles in smiles_list:\n",
    "    data, mol = smiles_to_graph(smiles, featurization_params)  # 传递 featurization_params\n",
    "    #print(data)\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "191f8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据移到CUDA设备\n",
    "data_list = [data.to(device) for data in data_list]\n",
    "mol_features = torch.tensor(mol_features, dtype=torch.float32).to(device)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.float32).to(device)\n",
    "\n",
    "# 创建模型并移动到GPU\n",
    "input_dim = 149  # 每个原子特征包含10个值\n",
    "hidden_dim = 300 #隐藏层\n",
    "output_dim = 1  # LD50预测\n",
    "num_mol_features = mol_features.shape[1]  # 分子特征的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0083272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_MLP_Model(input_dim, hidden_dim, output_dim, num_mol_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e7d69b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x28b083a97f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#钩子函数，显示每一行输出内容\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    print(f\"Module {module.__class__.__name__},Input:{input}, Output: {output}\")\n",
    "model.conv1.register_forward_hook(hook_fn)\n",
    "model.conv3.register_forward_hook(hook_fn)\n",
    "model.fc1.register_forward_hook(hook_fn)\n",
    "model.fc3.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b235f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fold 1/10...\n",
      "Epoch 1/100, Train Loss: 0.0685, Validation Loss: 0.0377\n",
      "loss: 2.000012859600247 2.000012859600247\n",
      "Epoch 2/100, Train Loss: 0.0380, Validation Loss: 0.0343\n",
      "loss: 1.8186491828091675 1.8186491828091675\n",
      "Epoch 3/100, Train Loss: 0.0342, Validation Loss: 0.0338\n",
      "loss: 1.7895522166363662 1.7895522166363662\n",
      "Epoch 4/100, Train Loss: 0.0315, Validation Loss: 0.0331\n",
      "loss: 1.756141127923911 1.756141127923911\n",
      "Epoch 5/100, Train Loss: 0.0293, Validation Loss: 0.0324\n",
      "loss: 1.716743938183754 1.716743938183754\n",
      "Epoch 6/100, Train Loss: 0.0275, Validation Loss: 0.0317\n",
      "loss: 1.6798574397062112 1.6798574397062112\n",
      "Epoch 7/100, Train Loss: 0.0260, Validation Loss: 0.0311\n",
      "loss: 1.6503468796072411 1.6503468796072411\n",
      "Epoch 8/100, Train Loss: 0.0245, Validation Loss: 0.0305\n",
      "loss: 1.6188391188625246 1.6188391188625246\n",
      "Epoch 9/100, Train Loss: 0.0231, Validation Loss: 0.0302\n",
      "loss: 1.597995170901413 1.597995170901413\n",
      "Epoch 10/100, Train Loss: 0.0218, Validation Loss: 0.0298\n",
      "loss: 1.5807852029211062 1.5807852029211062\n",
      "Epoch 11/100, Train Loss: 0.0205, Validation Loss: 0.0297\n",
      "loss: 1.5736075603417703 1.5736075603417703\n",
      "Epoch 12/100, Train Loss: 0.0193, Validation Loss: 0.0297\n",
      "Epoch 13/100, Train Loss: 0.0181, Validation Loss: 0.0299\n",
      "Epoch 14/100, Train Loss: 0.0170, Validation Loss: 0.0300\n",
      "Epoch 15/100, Train Loss: 0.0159, Validation Loss: 0.0304\n",
      "Epoch 16/100, Train Loss: 0.0149, Validation Loss: 0.0307\n",
      "Epoch 17/100, Train Loss: 0.0140, Validation Loss: 0.0306\n",
      "Epoch 18/100, Train Loss: 0.0132, Validation Loss: 0.0308\n",
      "Epoch 19/100, Train Loss: 0.0124, Validation Loss: 0.0309\n",
      "Epoch 20/100, Train Loss: 0.0117, Validation Loss: 0.0309\n",
      "Epoch 21/100, Train Loss: 0.0111, Validation Loss: 0.0310\n",
      "Epoch 22/100, Train Loss: 0.0104, Validation Loss: 0.0307\n",
      "Epoch 00022: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 23/100, Train Loss: 0.0097, Validation Loss: 0.0292\n",
      "loss: 1.5497325601820648 1.5497325601820648\n",
      "Epoch 24/100, Train Loss: 0.0092, Validation Loss: 0.0291\n",
      "loss: 1.544289878226266 1.544289878226266\n",
      "Epoch 25/100, Train Loss: 0.0087, Validation Loss: 0.0287\n",
      "loss: 1.5220766572654156 1.5220766572654156\n",
      "Epoch 26/100, Train Loss: 0.0083, Validation Loss: 0.0287\n",
      "Epoch 27/100, Train Loss: 0.0079, Validation Loss: 0.0287\n",
      "loss: 1.5188961600763378 1.5188961600763378\n",
      "Epoch 28/100, Train Loss: 0.0075, Validation Loss: 0.0286\n",
      "loss: 1.5161478941745372 1.5161478941745372\n",
      "Epoch 29/100, Train Loss: 0.0071, Validation Loss: 0.0284\n",
      "loss: 1.5036507814929791 1.5036507814929791\n",
      "Epoch 30/100, Train Loss: 0.0068, Validation Loss: 0.0282\n",
      "loss: 1.4921323741532433 1.4921323741532433\n",
      "Epoch 31/100, Train Loss: 0.0064, Validation Loss: 0.0281\n",
      "loss: 1.4875772174591475 1.4875772174591475\n",
      "Epoch 32/100, Train Loss: 0.0061, Validation Loss: 0.0279\n",
      "loss: 1.4800455493557365 1.4800455493557365\n",
      "Epoch 33/100, Train Loss: 0.0058, Validation Loss: 0.0280\n",
      "Epoch 34/100, Train Loss: 0.0055, Validation Loss: 0.0279\n",
      "Epoch 35/100, Train Loss: 0.0053, Validation Loss: 0.0278\n",
      "loss: 1.4726987227859354 1.4726987227859354\n",
      "Epoch 36/100, Train Loss: 0.0050, Validation Loss: 0.0279\n",
      "Epoch 37/100, Train Loss: 0.0048, Validation Loss: 0.0277\n",
      "loss: 1.468488772507044 1.468488772507044\n",
      "Epoch 38/100, Train Loss: 0.0046, Validation Loss: 0.0278\n",
      "Epoch 39/100, Train Loss: 0.0044, Validation Loss: 0.0278\n",
      "Epoch 40/100, Train Loss: 0.0042, Validation Loss: 0.0280\n",
      "Epoch 41/100, Train Loss: 0.0041, Validation Loss: 0.0279\n",
      "Epoch 42/100, Train Loss: 0.0039, Validation Loss: 0.0280\n",
      "Epoch 43/100, Train Loss: 0.0038, Validation Loss: 0.0281\n",
      "Epoch 44/100, Train Loss: 0.0036, Validation Loss: 0.0280\n",
      "Epoch 45/100, Train Loss: 0.0035, Validation Loss: 0.0282\n",
      "Epoch 46/100, Train Loss: 0.0034, Validation Loss: 0.0279\n",
      "Epoch 47/100, Train Loss: 0.0032, Validation Loss: 0.0281\n",
      "Epoch 48/100, Train Loss: 0.0031, Validation Loss: 0.0281\n",
      "Epoch 00048: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 49/100, Train Loss: 0.0030, Validation Loss: 0.0281\n",
      "Epoch 50/100, Train Loss: 0.0029, Validation Loss: 0.0281\n",
      "Epoch 51/100, Train Loss: 0.0028, Validation Loss: 0.0281\n",
      "Epoch 52/100, Train Loss: 0.0027, Validation Loss: 0.0282\n",
      "Epoch 53/100, Train Loss: 0.0026, Validation Loss: 0.0283\n",
      "Epoch 54/100, Train Loss: 0.0025, Validation Loss: 0.0282\n",
      "Epoch 55/100, Train Loss: 0.0025, Validation Loss: 0.0283\n",
      "Epoch 56/100, Train Loss: 0.0024, Validation Loss: 0.0283\n",
      "Epoch 57/100, Train Loss: 0.0023, Validation Loss: 0.0283\n",
      "Epoch 58/100, Train Loss: 0.0023, Validation Loss: 0.0285\n",
      "Epoch 59/100, Train Loss: 0.0022, Validation Loss: 0.0283\n",
      "Epoch 00059: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 60/100, Train Loss: 0.0021, Validation Loss: 0.0286\n",
      "Epoch 61/100, Train Loss: 0.0021, Validation Loss: 0.0286\n",
      "Epoch 62/100, Train Loss: 0.0020, Validation Loss: 0.0286\n",
      "Epoch 63/100, Train Loss: 0.0020, Validation Loss: 0.0285\n",
      "Epoch 64/100, Train Loss: 0.0019, Validation Loss: 0.0285\n",
      "Epoch 65/100, Train Loss: 0.0019, Validation Loss: 0.0285\n",
      "Epoch 66/100, Train Loss: 0.0018, Validation Loss: 0.0285\n",
      "Epoch 67/100, Train Loss: 0.0018, Validation Loss: 0.0286\n",
      "Epoch 68/100, Train Loss: 0.0018, Validation Loss: 0.0286\n",
      "Epoch 69/100, Train Loss: 0.0017, Validation Loss: 0.0283\n",
      "Epoch 70/100, Train Loss: 0.0017, Validation Loss: 0.0285\n",
      "Epoch 00070: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 71/100, Train Loss: 0.0017, Validation Loss: 0.0286\n",
      "Epoch 72/100, Train Loss: 0.0016, Validation Loss: 0.0285\n",
      "Epoch 73/100, Train Loss: 0.0016, Validation Loss: 0.0287\n",
      "Epoch 74/100, Train Loss: 0.0016, Validation Loss: 0.0288\n",
      "Epoch 75/100, Train Loss: 0.0016, Validation Loss: 0.0286\n",
      "Epoch 76/100, Train Loss: 0.0015, Validation Loss: 0.0284\n",
      "Epoch 77/100, Train Loss: 0.0015, Validation Loss: 0.0285\n",
      "Epoch 78/100, Train Loss: 0.0015, Validation Loss: 0.0284\n",
      "Epoch 79/100, Train Loss: 0.0015, Validation Loss: 0.0285\n",
      "Epoch 80/100, Train Loss: 0.0015, Validation Loss: 0.0283\n",
      "Epoch 81/100, Train Loss: 0.0014, Validation Loss: 0.0285\n",
      "Epoch 00081: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 82/100, Train Loss: 0.0014, Validation Loss: 0.0286\n",
      "Epoch 83/100, Train Loss: 0.0014, Validation Loss: 0.0284\n",
      "Epoch 84/100, Train Loss: 0.0014, Validation Loss: 0.0284\n",
      "Epoch 85/100, Train Loss: 0.0014, Validation Loss: 0.0285\n",
      "Epoch 86/100, Train Loss: 0.0013, Validation Loss: 0.0284\n",
      "Epoch 87/100, Train Loss: 0.0013, Validation Loss: 0.0283\n",
      "Epoch 88/100, Train Loss: 0.0013, Validation Loss: 0.0284\n",
      "Epoch 89/100, Train Loss: 0.0013, Validation Loss: 0.0282\n",
      "Epoch 90/100, Train Loss: 0.0013, Validation Loss: 0.0282\n",
      "Epoch 91/100, Train Loss: 0.0013, Validation Loss: 0.0282\n",
      "Epoch 92/100, Train Loss: 0.0013, Validation Loss: 0.0282\n",
      "Epoch 00092: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 93/100, Train Loss: 0.0012, Validation Loss: 0.0284\n",
      "Epoch 94/100, Train Loss: 0.0012, Validation Loss: 0.0283\n",
      "Epoch 95/100, Train Loss: 0.0012, Validation Loss: 0.0282\n",
      "Epoch 96/100, Train Loss: 0.0012, Validation Loss: 0.0283\n",
      "Epoch 97/100, Train Loss: 0.0012, Validation Loss: 0.0281\n",
      "Epoch 98/100, Train Loss: 0.0012, Validation Loss: 0.0280\n",
      "Epoch 99/100, Train Loss: 0.0012, Validation Loss: 0.0280\n",
      "Epoch 100/100, Train Loss: 0.0012, Validation Loss: 0.0280\n",
      "Fold 1 -> RMSE: 0.1665, MAE: 0.1222, R²: 0.1888\n",
      "\n",
      "Training Fold 2/10...\n",
      "Epoch 1/100, Train Loss: 0.0337, Validation Loss: 0.0193\n",
      "loss: 1.0242677642127092 1.0242677642127092\n",
      "Epoch 2/100, Train Loss: 0.0119, Validation Loss: 0.0145\n",
      "loss: 0.7694154447144683 0.7694154447144683\n",
      "Epoch 3/100, Train Loss: 0.0083, Validation Loss: 0.0140\n",
      "loss: 0.7400757660234376 0.7400757660234376\n",
      "Epoch 4/100, Train Loss: 0.0070, Validation Loss: 0.0136\n",
      "loss: 0.7210305184867138 0.7210305184867138\n",
      "Epoch 5/100, Train Loss: 0.0062, Validation Loss: 0.0134\n",
      "loss: 0.7112555215262546 0.7112555215262546\n",
      "Epoch 6/100, Train Loss: 0.0056, Validation Loss: 0.0134\n",
      "Epoch 7/100, Train Loss: 0.0052, Validation Loss: 0.0132\n",
      "loss: 0.6978088296400529 0.6978088296400529\n",
      "Epoch 8/100, Train Loss: 0.0049, Validation Loss: 0.0133\n",
      "Epoch 9/100, Train Loss: 0.0046, Validation Loss: 0.0133\n",
      "Epoch 10/100, Train Loss: 0.0043, Validation Loss: 0.0134\n",
      "Epoch 11/100, Train Loss: 0.0041, Validation Loss: 0.0132\n",
      "loss: 0.6975900270799684 0.6975900270799684\n",
      "Epoch 12/100, Train Loss: 0.0039, Validation Loss: 0.0131\n",
      "loss: 0.6958666137779801 0.6958666137779801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 0.0037, Validation Loss: 0.0133\n",
      "Epoch 14/100, Train Loss: 0.0035, Validation Loss: 0.0132\n",
      "Epoch 15/100, Train Loss: 0.0034, Validation Loss: 0.0130\n",
      "loss: 0.6875506194046466 0.6875506194046466\n",
      "Epoch 16/100, Train Loss: 0.0033, Validation Loss: 0.0130\n",
      "loss: 0.6866101755165346 0.6866101755165346\n",
      "Epoch 17/100, Train Loss: 0.0031, Validation Loss: 0.0130\n",
      "Epoch 18/100, Train Loss: 0.0030, Validation Loss: 0.0131\n",
      "Epoch 19/100, Train Loss: 0.0029, Validation Loss: 0.0131\n",
      "Epoch 20/100, Train Loss: 0.0028, Validation Loss: 0.0133\n",
      "Epoch 21/100, Train Loss: 0.0027, Validation Loss: 0.0130\n",
      "Epoch 22/100, Train Loss: 0.0027, Validation Loss: 0.0134\n",
      "Epoch 23/100, Train Loss: 0.0026, Validation Loss: 0.0132\n",
      "Epoch 24/100, Train Loss: 0.0025, Validation Loss: 0.0132\n",
      "Epoch 25/100, Train Loss: 0.0024, Validation Loss: 0.0133\n",
      "Epoch 26/100, Train Loss: 0.0024, Validation Loss: 0.0135\n",
      "Epoch 27/100, Train Loss: 0.0023, Validation Loss: 0.0131\n",
      "Epoch 00027: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 28/100, Train Loss: 0.0022, Validation Loss: 0.0132\n",
      "Epoch 29/100, Train Loss: 0.0022, Validation Loss: 0.0133\n",
      "Epoch 30/100, Train Loss: 0.0021, Validation Loss: 0.0130\n",
      "Epoch 31/100, Train Loss: 0.0021, Validation Loss: 0.0134\n",
      "Epoch 32/100, Train Loss: 0.0020, Validation Loss: 0.0131\n",
      "Epoch 33/100, Train Loss: 0.0020, Validation Loss: 0.0133\n",
      "Epoch 34/100, Train Loss: 0.0020, Validation Loss: 0.0133\n",
      "Epoch 35/100, Train Loss: 0.0019, Validation Loss: 0.0134\n",
      "Epoch 36/100, Train Loss: 0.0019, Validation Loss: 0.0133\n",
      "Epoch 37/100, Train Loss: 0.0019, Validation Loss: 0.0134\n",
      "Epoch 38/100, Train Loss: 0.0019, Validation Loss: 0.0134\n",
      "Epoch 00038: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 39/100, Train Loss: 0.0018, Validation Loss: 0.0135\n",
      "Epoch 40/100, Train Loss: 0.0018, Validation Loss: 0.0136\n",
      "Epoch 41/100, Train Loss: 0.0017, Validation Loss: 0.0137\n",
      "Epoch 42/100, Train Loss: 0.0017, Validation Loss: 0.0137\n",
      "Epoch 43/100, Train Loss: 0.0017, Validation Loss: 0.0137\n",
      "Epoch 44/100, Train Loss: 0.0017, Validation Loss: 0.0138\n",
      "Epoch 45/100, Train Loss: 0.0017, Validation Loss: 0.0138\n",
      "Epoch 46/100, Train Loss: 0.0016, Validation Loss: 0.0139\n",
      "Epoch 47/100, Train Loss: 0.0016, Validation Loss: 0.0139\n",
      "Epoch 48/100, Train Loss: 0.0016, Validation Loss: 0.0139\n",
      "Epoch 49/100, Train Loss: 0.0016, Validation Loss: 0.0140\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 50/100, Train Loss: 0.0016, Validation Loss: 0.0141\n",
      "Epoch 51/100, Train Loss: 0.0015, Validation Loss: 0.0141\n",
      "Epoch 52/100, Train Loss: 0.0015, Validation Loss: 0.0142\n",
      "Epoch 53/100, Train Loss: 0.0015, Validation Loss: 0.0143\n",
      "Epoch 54/100, Train Loss: 0.0015, Validation Loss: 0.0142\n",
      "Epoch 55/100, Train Loss: 0.0015, Validation Loss: 0.0144\n",
      "Epoch 56/100, Train Loss: 0.0014, Validation Loss: 0.0143\n",
      "Epoch 57/100, Train Loss: 0.0014, Validation Loss: 0.0143\n",
      "Epoch 58/100, Train Loss: 0.0014, Validation Loss: 0.0144\n",
      "Epoch 59/100, Train Loss: 0.0014, Validation Loss: 0.0144\n",
      "Epoch 60/100, Train Loss: 0.0014, Validation Loss: 0.0145\n",
      "Epoch 00060: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 61/100, Train Loss: 0.0014, Validation Loss: 0.0145\n",
      "Epoch 62/100, Train Loss: 0.0014, Validation Loss: 0.0146\n",
      "Epoch 63/100, Train Loss: 0.0014, Validation Loss: 0.0146\n",
      "Epoch 64/100, Train Loss: 0.0013, Validation Loss: 0.0146\n",
      "Epoch 65/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 66/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 67/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 68/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 69/100, Train Loss: 0.0013, Validation Loss: 0.0148\n",
      "Epoch 70/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 71/100, Train Loss: 0.0013, Validation Loss: 0.0147\n",
      "Epoch 00071: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 72/100, Train Loss: 0.0013, Validation Loss: 0.0149\n",
      "Epoch 73/100, Train Loss: 0.0012, Validation Loss: 0.0149\n",
      "Epoch 74/100, Train Loss: 0.0012, Validation Loss: 0.0149\n",
      "Epoch 75/100, Train Loss: 0.0012, Validation Loss: 0.0149\n",
      "Epoch 76/100, Train Loss: 0.0012, Validation Loss: 0.0150\n",
      "Epoch 77/100, Train Loss: 0.0012, Validation Loss: 0.0150\n",
      "Epoch 78/100, Train Loss: 0.0012, Validation Loss: 0.0149\n",
      "Epoch 79/100, Train Loss: 0.0012, Validation Loss: 0.0150\n",
      "Epoch 80/100, Train Loss: 0.0012, Validation Loss: 0.0150\n",
      "Epoch 81/100, Train Loss: 0.0012, Validation Loss: 0.0150\n",
      "Epoch 82/100, Train Loss: 0.0012, Validation Loss: 0.0149\n",
      "Epoch 00082: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 83/100, Train Loss: 0.0012, Validation Loss: 0.0151\n",
      "Epoch 84/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 85/100, Train Loss: 0.0011, Validation Loss: 0.0150\n",
      "Epoch 86/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 87/100, Train Loss: 0.0011, Validation Loss: 0.0150\n",
      "Epoch 88/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 89/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 90/100, Train Loss: 0.0011, Validation Loss: 0.0150\n",
      "Epoch 91/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 92/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 93/100, Train Loss: 0.0011, Validation Loss: 0.0152\n",
      "Epoch 00093: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 94/100, Train Loss: 0.0011, Validation Loss: 0.0151\n",
      "Epoch 95/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Epoch 96/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Epoch 97/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Epoch 98/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Epoch 99/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Epoch 100/100, Train Loss: 0.0010, Validation Loss: 0.0152\n",
      "Fold 2 -> RMSE: 0.1138, MAE: 0.0847, R²: 0.7153\n",
      "\n",
      "Training Fold 3/10...\n",
      "Epoch 1/100, Train Loss: 0.0264, Validation Loss: 0.0132\n",
      "loss: 0.6863184083238139 0.6863184083238139\n",
      "Epoch 2/100, Train Loss: 0.0092, Validation Loss: 0.0095\n",
      "loss: 0.49343566792595084 0.49343566792595084\n",
      "Epoch 3/100, Train Loss: 0.0059, Validation Loss: 0.0090\n",
      "loss: 0.4663518109639142 0.4663518109639142\n",
      "Epoch 4/100, Train Loss: 0.0048, Validation Loss: 0.0087\n",
      "loss: 0.4502509905323677 0.4502509905323677\n",
      "Epoch 5/100, Train Loss: 0.0042, Validation Loss: 0.0090\n",
      "Epoch 6/100, Train Loss: 0.0038, Validation Loss: 0.0088\n",
      "Epoch 7/100, Train Loss: 0.0035, Validation Loss: 0.0092\n",
      "Epoch 8/100, Train Loss: 0.0034, Validation Loss: 0.0089\n",
      "Epoch 9/100, Train Loss: 0.0032, Validation Loss: 0.0091\n",
      "Epoch 10/100, Train Loss: 0.0030, Validation Loss: 0.0089\n",
      "Epoch 11/100, Train Loss: 0.0029, Validation Loss: 0.0091\n",
      "Epoch 12/100, Train Loss: 0.0027, Validation Loss: 0.0089\n",
      "Epoch 13/100, Train Loss: 0.0026, Validation Loss: 0.0089\n",
      "Epoch 14/100, Train Loss: 0.0025, Validation Loss: 0.0087\n",
      "Epoch 15/100, Train Loss: 0.0024, Validation Loss: 0.0086\n",
      "loss: 0.4495377937205376 0.4495377937205376\n",
      "Epoch 16/100, Train Loss: 0.0023, Validation Loss: 0.0088\n",
      "Epoch 17/100, Train Loss: 0.0023, Validation Loss: 0.0086\n",
      "loss: 0.44571283314712673 0.44571283314712673\n",
      "Epoch 18/100, Train Loss: 0.0022, Validation Loss: 0.0088\n",
      "Epoch 19/100, Train Loss: 0.0021, Validation Loss: 0.0086\n",
      "loss: 0.4446312021120775 0.4446312021120775\n",
      "Epoch 20/100, Train Loss: 0.0021, Validation Loss: 0.0085\n",
      "loss: 0.4422937756455667 0.4422937756455667\n",
      "Epoch 21/100, Train Loss: 0.0020, Validation Loss: 0.0089\n",
      "Epoch 22/100, Train Loss: 0.0020, Validation Loss: 0.0084\n",
      "loss: 0.435117550510995 0.435117550510995\n",
      "Epoch 23/100, Train Loss: 0.0019, Validation Loss: 0.0086\n",
      "Epoch 24/100, Train Loss: 0.0018, Validation Loss: 0.0083\n",
      "loss: 0.43291598858559155 0.43291598858559155\n",
      "Epoch 25/100, Train Loss: 0.0018, Validation Loss: 0.0085\n",
      "Epoch 26/100, Train Loss: 0.0018, Validation Loss: 0.0083\n",
      "loss: 0.429323369165477 0.429323369165477\n",
      "Epoch 27/100, Train Loss: 0.0017, Validation Loss: 0.0084\n",
      "Epoch 28/100, Train Loss: 0.0017, Validation Loss: 0.0083\n",
      "Epoch 29/100, Train Loss: 0.0016, Validation Loss: 0.0084\n",
      "Epoch 30/100, Train Loss: 0.0016, Validation Loss: 0.0081\n",
      "loss: 0.42232570702844896 0.42232570702844896\n",
      "Epoch 31/100, Train Loss: 0.0016, Validation Loss: 0.0083\n",
      "Epoch 32/100, Train Loss: 0.0016, Validation Loss: 0.0083\n",
      "Epoch 33/100, Train Loss: 0.0015, Validation Loss: 0.0082\n",
      "Epoch 34/100, Train Loss: 0.0015, Validation Loss: 0.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Train Loss: 0.0015, Validation Loss: 0.0082\n",
      "Epoch 36/100, Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch 37/100, Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch 38/100, Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch 39/100, Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch 40/100, Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch 41/100, Train Loss: 0.0013, Validation Loss: 0.0082\n",
      "Epoch 00041: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 42/100, Train Loss: 0.0013, Validation Loss: 0.0082\n",
      "Epoch 43/100, Train Loss: 0.0013, Validation Loss: 0.0081\n",
      "loss: 0.4222891373127027 0.4222891373127027\n",
      "Epoch 44/100, Train Loss: 0.0012, Validation Loss: 0.0081\n",
      "loss: 0.42019713227364264 0.42019713227364264\n",
      "Epoch 45/100, Train Loss: 0.0012, Validation Loss: 0.0082\n",
      "Epoch 46/100, Train Loss: 0.0012, Validation Loss: 0.0082\n",
      "Epoch 47/100, Train Loss: 0.0012, Validation Loss: 0.0082\n",
      "Epoch 48/100, Train Loss: 0.0012, Validation Loss: 0.0083\n",
      "Epoch 49/100, Train Loss: 0.0012, Validation Loss: 0.0082\n",
      "Epoch 50/100, Train Loss: 0.0011, Validation Loss: 0.0083\n",
      "Epoch 51/100, Train Loss: 0.0011, Validation Loss: 0.0083\n",
      "Epoch 52/100, Train Loss: 0.0011, Validation Loss: 0.0084\n",
      "Epoch 53/100, Train Loss: 0.0011, Validation Loss: 0.0083\n",
      "Epoch 54/100, Train Loss: 0.0011, Validation Loss: 0.0083\n",
      "Epoch 55/100, Train Loss: 0.0011, Validation Loss: 0.0084\n",
      "Epoch 00055: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 56/100, Train Loss: 0.0011, Validation Loss: 0.0084\n",
      "Epoch 57/100, Train Loss: 0.0011, Validation Loss: 0.0083\n",
      "Epoch 58/100, Train Loss: 0.0010, Validation Loss: 0.0084\n",
      "Epoch 59/100, Train Loss: 0.0010, Validation Loss: 0.0084\n",
      "Epoch 60/100, Train Loss: 0.0010, Validation Loss: 0.0084\n",
      "Epoch 61/100, Train Loss: 0.0010, Validation Loss: 0.0085\n",
      "Epoch 62/100, Train Loss: 0.0010, Validation Loss: 0.0085\n",
      "Epoch 63/100, Train Loss: 0.0010, Validation Loss: 0.0084\n",
      "Epoch 64/100, Train Loss: 0.0010, Validation Loss: 0.0085\n",
      "Epoch 65/100, Train Loss: 0.0010, Validation Loss: 0.0085\n",
      "Epoch 66/100, Train Loss: 0.0010, Validation Loss: 0.0085\n",
      "Epoch 00066: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 67/100, Train Loss: 0.0010, Validation Loss: 0.0086\n",
      "Epoch 68/100, Train Loss: 0.0010, Validation Loss: 0.0086\n",
      "Epoch 69/100, Train Loss: 0.0009, Validation Loss: 0.0085\n",
      "Epoch 70/100, Train Loss: 0.0009, Validation Loss: 0.0086\n",
      "Epoch 71/100, Train Loss: 0.0009, Validation Loss: 0.0086\n",
      "Epoch 72/100, Train Loss: 0.0009, Validation Loss: 0.0086\n",
      "Epoch 73/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 74/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 75/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 76/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 77/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 00077: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 78/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 79/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 80/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 81/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 82/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 83/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 84/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 85/100, Train Loss: 0.0009, Validation Loss: 0.0089\n",
      "Epoch 86/100, Train Loss: 0.0009, Validation Loss: 0.0089\n",
      "Epoch 87/100, Train Loss: 0.0009, Validation Loss: 0.0089\n",
      "Epoch 88/100, Train Loss: 0.0008, Validation Loss: 0.0089\n",
      "Epoch 00088: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 89/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 90/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 91/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 92/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 93/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 94/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 95/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 96/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 97/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 98/100, Train Loss: 0.0008, Validation Loss: 0.0091\n",
      "Epoch 99/100, Train Loss: 0.0008, Validation Loss: 0.0091\n",
      "Epoch 00099: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 100/100, Train Loss: 0.0008, Validation Loss: 0.0091\n",
      "Fold 3 -> RMSE: 0.0899, MAE: 0.0660, R²: 0.8478\n",
      "\n",
      "Training Fold 4/10...\n",
      "Epoch 1/100, Train Loss: 0.0283, Validation Loss: 0.0141\n",
      "loss: 0.7338951934584657 0.7338951934584657\n",
      "Epoch 2/100, Train Loss: 0.0074, Validation Loss: 0.0071\n",
      "loss: 0.3711697592201517 0.3711697592201517\n",
      "Epoch 3/100, Train Loss: 0.0038, Validation Loss: 0.0045\n",
      "loss: 0.23458512649560603 0.23458512649560603\n",
      "Epoch 4/100, Train Loss: 0.0030, Validation Loss: 0.0037\n",
      "loss: 0.19204534646157256 0.19204534646157256\n",
      "Epoch 5/100, Train Loss: 0.0026, Validation Loss: 0.0034\n",
      "loss: 0.1762103506305266 0.1762103506305266\n",
      "Epoch 6/100, Train Loss: 0.0023, Validation Loss: 0.0031\n",
      "loss: 0.15951961966197814 0.15951961966197814\n",
      "Epoch 7/100, Train Loss: 0.0021, Validation Loss: 0.0029\n",
      "loss: 0.15269453677075262 0.15269453677075262\n",
      "Epoch 8/100, Train Loss: 0.0020, Validation Loss: 0.0030\n",
      "Epoch 9/100, Train Loss: 0.0019, Validation Loss: 0.0029\n",
      "loss: 0.15196257364583232 0.15196257364583232\n",
      "Epoch 10/100, Train Loss: 0.0018, Validation Loss: 0.0030\n",
      "Epoch 11/100, Train Loss: 0.0018, Validation Loss: 0.0029\n",
      "Epoch 12/100, Train Loss: 0.0017, Validation Loss: 0.0030\n",
      "Epoch 13/100, Train Loss: 0.0016, Validation Loss: 0.0030\n",
      "Epoch 14/100, Train Loss: 0.0016, Validation Loss: 0.0030\n",
      "Epoch 15/100, Train Loss: 0.0016, Validation Loss: 0.0030\n",
      "Epoch 16/100, Train Loss: 0.0015, Validation Loss: 0.0031\n",
      "Epoch 17/100, Train Loss: 0.0015, Validation Loss: 0.0031\n",
      "Epoch 18/100, Train Loss: 0.0015, Validation Loss: 0.0031\n",
      "Epoch 19/100, Train Loss: 0.0014, Validation Loss: 0.0031\n",
      "Epoch 20/100, Train Loss: 0.0014, Validation Loss: 0.0031\n",
      "Epoch 00020: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 21/100, Train Loss: 0.0014, Validation Loss: 0.0032\n",
      "Epoch 22/100, Train Loss: 0.0013, Validation Loss: 0.0032\n",
      "Epoch 23/100, Train Loss: 0.0013, Validation Loss: 0.0032\n",
      "Epoch 24/100, Train Loss: 0.0013, Validation Loss: 0.0032\n",
      "Epoch 25/100, Train Loss: 0.0013, Validation Loss: 0.0033\n",
      "Epoch 26/100, Train Loss: 0.0013, Validation Loss: 0.0033\n",
      "Epoch 27/100, Train Loss: 0.0013, Validation Loss: 0.0033\n",
      "Epoch 28/100, Train Loss: 0.0012, Validation Loss: 0.0033\n",
      "Epoch 29/100, Train Loss: 0.0012, Validation Loss: 0.0034\n",
      "Epoch 30/100, Train Loss: 0.0012, Validation Loss: 0.0034\n",
      "Epoch 31/100, Train Loss: 0.0012, Validation Loss: 0.0034\n",
      "Epoch 00031: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 32/100, Train Loss: 0.0012, Validation Loss: 0.0034\n",
      "Epoch 33/100, Train Loss: 0.0012, Validation Loss: 0.0034\n",
      "Epoch 34/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 35/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 36/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 37/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 38/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 39/100, Train Loss: 0.0011, Validation Loss: 0.0036\n",
      "Epoch 40/100, Train Loss: 0.0011, Validation Loss: 0.0035\n",
      "Epoch 41/100, Train Loss: 0.0011, Validation Loss: 0.0036\n",
      "Epoch 42/100, Train Loss: 0.0011, Validation Loss: 0.0036\n",
      "Epoch 00042: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 43/100, Train Loss: 0.0011, Validation Loss: 0.0037\n",
      "Epoch 44/100, Train Loss: 0.0011, Validation Loss: 0.0037\n",
      "Epoch 45/100, Train Loss: 0.0010, Validation Loss: 0.0037\n",
      "Epoch 46/100, Train Loss: 0.0010, Validation Loss: 0.0037\n",
      "Epoch 47/100, Train Loss: 0.0010, Validation Loss: 0.0037\n",
      "Epoch 48/100, Train Loss: 0.0010, Validation Loss: 0.0038\n",
      "Epoch 49/100, Train Loss: 0.0010, Validation Loss: 0.0038\n",
      "Epoch 50/100, Train Loss: 0.0010, Validation Loss: 0.0038\n",
      "Epoch 51/100, Train Loss: 0.0010, Validation Loss: 0.0038\n",
      "Epoch 52/100, Train Loss: 0.0010, Validation Loss: 0.0039\n",
      "Epoch 53/100, Train Loss: 0.0010, Validation Loss: 0.0039\n",
      "Epoch 00053: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 54/100, Train Loss: 0.0010, Validation Loss: 0.0039\n",
      "Epoch 55/100, Train Loss: 0.0010, Validation Loss: 0.0039\n",
      "Epoch 56/100, Train Loss: 0.0010, Validation Loss: 0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Train Loss: 0.0010, Validation Loss: 0.0040\n",
      "Epoch 58/100, Train Loss: 0.0010, Validation Loss: 0.0040\n",
      "Epoch 59/100, Train Loss: 0.0009, Validation Loss: 0.0040\n",
      "Epoch 60/100, Train Loss: 0.0009, Validation Loss: 0.0040\n",
      "Epoch 61/100, Train Loss: 0.0009, Validation Loss: 0.0040\n",
      "Epoch 62/100, Train Loss: 0.0009, Validation Loss: 0.0041\n",
      "Epoch 63/100, Train Loss: 0.0009, Validation Loss: 0.0041\n",
      "Epoch 64/100, Train Loss: 0.0009, Validation Loss: 0.0041\n",
      "Epoch 00064: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 65/100, Train Loss: 0.0009, Validation Loss: 0.0041\n",
      "Epoch 66/100, Train Loss: 0.0009, Validation Loss: 0.0042\n",
      "Epoch 67/100, Train Loss: 0.0009, Validation Loss: 0.0042\n",
      "Epoch 68/100, Train Loss: 0.0009, Validation Loss: 0.0042\n",
      "Epoch 69/100, Train Loss: 0.0009, Validation Loss: 0.0042\n",
      "Epoch 70/100, Train Loss: 0.0009, Validation Loss: 0.0043\n",
      "Epoch 71/100, Train Loss: 0.0009, Validation Loss: 0.0043\n",
      "Epoch 72/100, Train Loss: 0.0009, Validation Loss: 0.0043\n",
      "Epoch 73/100, Train Loss: 0.0009, Validation Loss: 0.0043\n",
      "Epoch 74/100, Train Loss: 0.0009, Validation Loss: 0.0044\n",
      "Epoch 75/100, Train Loss: 0.0009, Validation Loss: 0.0044\n",
      "Epoch 00075: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 76/100, Train Loss: 0.0008, Validation Loss: 0.0044\n",
      "Epoch 77/100, Train Loss: 0.0008, Validation Loss: 0.0044\n",
      "Epoch 78/100, Train Loss: 0.0008, Validation Loss: 0.0045\n",
      "Epoch 79/100, Train Loss: 0.0008, Validation Loss: 0.0045\n",
      "Epoch 80/100, Train Loss: 0.0008, Validation Loss: 0.0045\n",
      "Epoch 81/100, Train Loss: 0.0008, Validation Loss: 0.0046\n",
      "Epoch 82/100, Train Loss: 0.0008, Validation Loss: 0.0046\n",
      "Epoch 83/100, Train Loss: 0.0008, Validation Loss: 0.0046\n",
      "Epoch 84/100, Train Loss: 0.0008, Validation Loss: 0.0046\n",
      "Epoch 85/100, Train Loss: 0.0008, Validation Loss: 0.0046\n",
      "Epoch 86/100, Train Loss: 0.0008, Validation Loss: 0.0047\n",
      "Epoch 00086: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 87/100, Train Loss: 0.0008, Validation Loss: 0.0047\n",
      "Epoch 88/100, Train Loss: 0.0008, Validation Loss: 0.0047\n",
      "Epoch 89/100, Train Loss: 0.0008, Validation Loss: 0.0048\n",
      "Epoch 90/100, Train Loss: 0.0008, Validation Loss: 0.0048\n",
      "Epoch 91/100, Train Loss: 0.0008, Validation Loss: 0.0048\n",
      "Epoch 92/100, Train Loss: 0.0008, Validation Loss: 0.0048\n",
      "Epoch 93/100, Train Loss: 0.0008, Validation Loss: 0.0049\n",
      "Epoch 94/100, Train Loss: 0.0008, Validation Loss: 0.0049\n",
      "Epoch 95/100, Train Loss: 0.0008, Validation Loss: 0.0049\n",
      "Epoch 96/100, Train Loss: 0.0008, Validation Loss: 0.0050\n",
      "Epoch 97/100, Train Loss: 0.0008, Validation Loss: 0.0050\n",
      "Epoch 00097: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0050\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0050\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0051\n",
      "Fold 4 -> RMSE: 0.0541, MAE: 0.0414, R²: 0.9460\n",
      "\n",
      "Training Fold 5/10...\n",
      "Epoch 1/100, Train Loss: 0.0219, Validation Loss: 0.0130\n",
      "loss: 0.6747478436304846 0.6747478436304846\n",
      "Epoch 2/100, Train Loss: 0.0061, Validation Loss: 0.0071\n",
      "loss: 0.36675798063456355 0.36675798063456355\n",
      "Epoch 3/100, Train Loss: 0.0030, Validation Loss: 0.0063\n",
      "loss: 0.3267154128312768 0.3267154128312768\n",
      "Epoch 4/100, Train Loss: 0.0023, Validation Loss: 0.0059\n",
      "loss: 0.3093610049450177 0.3093610049450177\n",
      "Epoch 5/100, Train Loss: 0.0020, Validation Loss: 0.0059\n",
      "loss: 0.3071401268457521 0.3071401268457521\n",
      "Epoch 6/100, Train Loss: 0.0018, Validation Loss: 0.0059\n",
      "loss: 0.30685840957903565 0.30685840957903565\n",
      "Epoch 7/100, Train Loss: 0.0016, Validation Loss: 0.0061\n",
      "Epoch 8/100, Train Loss: 0.0015, Validation Loss: 0.0062\n",
      "Epoch 9/100, Train Loss: 0.0014, Validation Loss: 0.0063\n",
      "Epoch 10/100, Train Loss: 0.0014, Validation Loss: 0.0065\n",
      "Epoch 11/100, Train Loss: 0.0013, Validation Loss: 0.0065\n",
      "Epoch 12/100, Train Loss: 0.0013, Validation Loss: 0.0066\n",
      "Epoch 13/100, Train Loss: 0.0013, Validation Loss: 0.0067\n",
      "Epoch 14/100, Train Loss: 0.0012, Validation Loss: 0.0069\n",
      "Epoch 15/100, Train Loss: 0.0012, Validation Loss: 0.0070\n",
      "Epoch 16/100, Train Loss: 0.0012, Validation Loss: 0.0071\n",
      "Epoch 17/100, Train Loss: 0.0012, Validation Loss: 0.0072\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 18/100, Train Loss: 0.0012, Validation Loss: 0.0073\n",
      "Epoch 19/100, Train Loss: 0.0011, Validation Loss: 0.0073\n",
      "Epoch 20/100, Train Loss: 0.0011, Validation Loss: 0.0074\n",
      "Epoch 21/100, Train Loss: 0.0011, Validation Loss: 0.0075\n",
      "Epoch 22/100, Train Loss: 0.0011, Validation Loss: 0.0076\n",
      "Epoch 23/100, Train Loss: 0.0011, Validation Loss: 0.0077\n",
      "Epoch 24/100, Train Loss: 0.0011, Validation Loss: 0.0077\n",
      "Epoch 25/100, Train Loss: 0.0011, Validation Loss: 0.0078\n",
      "Epoch 26/100, Train Loss: 0.0010, Validation Loss: 0.0078\n",
      "Epoch 27/100, Train Loss: 0.0010, Validation Loss: 0.0079\n",
      "Epoch 28/100, Train Loss: 0.0010, Validation Loss: 0.0080\n",
      "Epoch 00028: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 29/100, Train Loss: 0.0010, Validation Loss: 0.0080\n",
      "Epoch 30/100, Train Loss: 0.0010, Validation Loss: 0.0081\n",
      "Epoch 31/100, Train Loss: 0.0010, Validation Loss: 0.0081\n",
      "Epoch 32/100, Train Loss: 0.0010, Validation Loss: 0.0082\n",
      "Epoch 33/100, Train Loss: 0.0010, Validation Loss: 0.0082\n",
      "Epoch 34/100, Train Loss: 0.0010, Validation Loss: 0.0083\n",
      "Epoch 35/100, Train Loss: 0.0009, Validation Loss: 0.0083\n",
      "Epoch 36/100, Train Loss: 0.0009, Validation Loss: 0.0084\n",
      "Epoch 37/100, Train Loss: 0.0009, Validation Loss: 0.0084\n",
      "Epoch 38/100, Train Loss: 0.0009, Validation Loss: 0.0085\n",
      "Epoch 39/100, Train Loss: 0.0009, Validation Loss: 0.0085\n",
      "Epoch 00039: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 40/100, Train Loss: 0.0009, Validation Loss: 0.0085\n",
      "Epoch 41/100, Train Loss: 0.0009, Validation Loss: 0.0086\n",
      "Epoch 42/100, Train Loss: 0.0009, Validation Loss: 0.0086\n",
      "Epoch 43/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 44/100, Train Loss: 0.0009, Validation Loss: 0.0087\n",
      "Epoch 45/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 46/100, Train Loss: 0.0009, Validation Loss: 0.0088\n",
      "Epoch 47/100, Train Loss: 0.0009, Validation Loss: 0.0089\n",
      "Epoch 48/100, Train Loss: 0.0009, Validation Loss: 0.0089\n",
      "Epoch 49/100, Train Loss: 0.0009, Validation Loss: 0.0090\n",
      "Epoch 50/100, Train Loss: 0.0009, Validation Loss: 0.0090\n",
      "Epoch 00050: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 51/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 52/100, Train Loss: 0.0008, Validation Loss: 0.0090\n",
      "Epoch 53/100, Train Loss: 0.0008, Validation Loss: 0.0091\n",
      "Epoch 54/100, Train Loss: 0.0008, Validation Loss: 0.0091\n",
      "Epoch 55/100, Train Loss: 0.0008, Validation Loss: 0.0092\n",
      "Epoch 56/100, Train Loss: 0.0008, Validation Loss: 0.0092\n",
      "Epoch 57/100, Train Loss: 0.0008, Validation Loss: 0.0092\n",
      "Epoch 58/100, Train Loss: 0.0008, Validation Loss: 0.0093\n",
      "Epoch 59/100, Train Loss: 0.0008, Validation Loss: 0.0093\n",
      "Epoch 60/100, Train Loss: 0.0008, Validation Loss: 0.0093\n",
      "Epoch 61/100, Train Loss: 0.0008, Validation Loss: 0.0094\n",
      "Epoch 00061: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 62/100, Train Loss: 0.0008, Validation Loss: 0.0094\n",
      "Epoch 63/100, Train Loss: 0.0008, Validation Loss: 0.0095\n",
      "Epoch 64/100, Train Loss: 0.0008, Validation Loss: 0.0095\n",
      "Epoch 65/100, Train Loss: 0.0008, Validation Loss: 0.0095\n",
      "Epoch 66/100, Train Loss: 0.0008, Validation Loss: 0.0096\n",
      "Epoch 67/100, Train Loss: 0.0007, Validation Loss: 0.0096\n",
      "Epoch 68/100, Train Loss: 0.0007, Validation Loss: 0.0097\n",
      "Epoch 69/100, Train Loss: 0.0007, Validation Loss: 0.0097\n",
      "Epoch 70/100, Train Loss: 0.0007, Validation Loss: 0.0097\n",
      "Epoch 71/100, Train Loss: 0.0007, Validation Loss: 0.0098\n",
      "Epoch 72/100, Train Loss: 0.0007, Validation Loss: 0.0098\n",
      "Epoch 00072: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 73/100, Train Loss: 0.0007, Validation Loss: 0.0099\n",
      "Epoch 74/100, Train Loss: 0.0007, Validation Loss: 0.0099\n",
      "Epoch 75/100, Train Loss: 0.0007, Validation Loss: 0.0099\n",
      "Epoch 76/100, Train Loss: 0.0007, Validation Loss: 0.0099\n",
      "Epoch 77/100, Train Loss: 0.0007, Validation Loss: 0.0100\n",
      "Epoch 78/100, Train Loss: 0.0007, Validation Loss: 0.0100\n",
      "Epoch 79/100, Train Loss: 0.0007, Validation Loss: 0.0100\n",
      "Epoch 80/100, Train Loss: 0.0007, Validation Loss: 0.0101\n",
      "Epoch 81/100, Train Loss: 0.0007, Validation Loss: 0.0101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Train Loss: 0.0007, Validation Loss: 0.0101\n",
      "Epoch 83/100, Train Loss: 0.0007, Validation Loss: 0.0102\n",
      "Epoch 00083: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 84/100, Train Loss: 0.0007, Validation Loss: 0.0102\n",
      "Epoch 85/100, Train Loss: 0.0007, Validation Loss: 0.0102\n",
      "Epoch 86/100, Train Loss: 0.0007, Validation Loss: 0.0102\n",
      "Epoch 87/100, Train Loss: 0.0007, Validation Loss: 0.0103\n",
      "Epoch 88/100, Train Loss: 0.0007, Validation Loss: 0.0103\n",
      "Epoch 89/100, Train Loss: 0.0007, Validation Loss: 0.0103\n",
      "Epoch 90/100, Train Loss: 0.0007, Validation Loss: 0.0104\n",
      "Epoch 91/100, Train Loss: 0.0007, Validation Loss: 0.0104\n",
      "Epoch 92/100, Train Loss: 0.0007, Validation Loss: 0.0104\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0104\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0104\n",
      "Epoch 00094: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0105\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0105\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0105\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0106\n",
      "Epoch 99/100, Train Loss: 0.0006, Validation Loss: 0.0106\n",
      "Epoch 100/100, Train Loss: 0.0006, Validation Loss: 0.0106\n",
      "Fold 5 -> RMSE: 0.0768, MAE: 0.0532, R²: 0.8202\n",
      "\n",
      "Training Fold 6/10...\n",
      "Epoch 1/100, Train Loss: 0.0196, Validation Loss: 0.0091\n",
      "loss: 0.4722805847111431 0.4722805847111431\n",
      "Epoch 2/100, Train Loss: 0.0050, Validation Loss: 0.0039\n",
      "loss: 0.203139336201275 0.203139336201275\n",
      "Epoch 3/100, Train Loss: 0.0029, Validation Loss: 0.0035\n",
      "loss: 0.18112882087950766 0.18112882087950766\n",
      "Epoch 4/100, Train Loss: 0.0024, Validation Loss: 0.0035\n",
      "Epoch 5/100, Train Loss: 0.0021, Validation Loss: 0.0035\n",
      "Epoch 6/100, Train Loss: 0.0019, Validation Loss: 0.0037\n",
      "Epoch 7/100, Train Loss: 0.0018, Validation Loss: 0.0036\n",
      "Epoch 8/100, Train Loss: 0.0017, Validation Loss: 0.0036\n",
      "Epoch 9/100, Train Loss: 0.0017, Validation Loss: 0.0036\n",
      "Epoch 10/100, Train Loss: 0.0016, Validation Loss: 0.0036\n",
      "Epoch 11/100, Train Loss: 0.0016, Validation Loss: 0.0038\n",
      "Epoch 12/100, Train Loss: 0.0015, Validation Loss: 0.0037\n",
      "Epoch 13/100, Train Loss: 0.0015, Validation Loss: 0.0037\n",
      "Epoch 14/100, Train Loss: 0.0015, Validation Loss: 0.0039\n",
      "Epoch 00014: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 15/100, Train Loss: 0.0014, Validation Loss: 0.0037\n",
      "Epoch 16/100, Train Loss: 0.0014, Validation Loss: 0.0037\n",
      "Epoch 17/100, Train Loss: 0.0013, Validation Loss: 0.0038\n",
      "Epoch 18/100, Train Loss: 0.0013, Validation Loss: 0.0038\n",
      "Epoch 19/100, Train Loss: 0.0013, Validation Loss: 0.0040\n",
      "Epoch 20/100, Train Loss: 0.0013, Validation Loss: 0.0039\n",
      "Epoch 21/100, Train Loss: 0.0013, Validation Loss: 0.0040\n",
      "Epoch 22/100, Train Loss: 0.0013, Validation Loss: 0.0040\n",
      "Epoch 23/100, Train Loss: 0.0012, Validation Loss: 0.0041\n",
      "Epoch 24/100, Train Loss: 0.0012, Validation Loss: 0.0041\n",
      "Epoch 25/100, Train Loss: 0.0012, Validation Loss: 0.0042\n",
      "Epoch 00025: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 26/100, Train Loss: 0.0012, Validation Loss: 0.0041\n",
      "Epoch 27/100, Train Loss: 0.0012, Validation Loss: 0.0042\n",
      "Epoch 28/100, Train Loss: 0.0011, Validation Loss: 0.0042\n",
      "Epoch 29/100, Train Loss: 0.0011, Validation Loss: 0.0043\n",
      "Epoch 30/100, Train Loss: 0.0011, Validation Loss: 0.0043\n",
      "Epoch 31/100, Train Loss: 0.0011, Validation Loss: 0.0044\n",
      "Epoch 32/100, Train Loss: 0.0011, Validation Loss: 0.0044\n",
      "Epoch 33/100, Train Loss: 0.0011, Validation Loss: 0.0044\n",
      "Epoch 34/100, Train Loss: 0.0011, Validation Loss: 0.0045\n",
      "Epoch 35/100, Train Loss: 0.0011, Validation Loss: 0.0045\n",
      "Epoch 36/100, Train Loss: 0.0011, Validation Loss: 0.0046\n",
      "Epoch 00036: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 37/100, Train Loss: 0.0010, Validation Loss: 0.0046\n",
      "Epoch 38/100, Train Loss: 0.0010, Validation Loss: 0.0046\n",
      "Epoch 39/100, Train Loss: 0.0010, Validation Loss: 0.0047\n",
      "Epoch 40/100, Train Loss: 0.0010, Validation Loss: 0.0046\n",
      "Epoch 41/100, Train Loss: 0.0010, Validation Loss: 0.0047\n",
      "Epoch 42/100, Train Loss: 0.0010, Validation Loss: 0.0048\n",
      "Epoch 43/100, Train Loss: 0.0010, Validation Loss: 0.0048\n",
      "Epoch 44/100, Train Loss: 0.0010, Validation Loss: 0.0048\n",
      "Epoch 45/100, Train Loss: 0.0010, Validation Loss: 0.0049\n",
      "Epoch 46/100, Train Loss: 0.0010, Validation Loss: 0.0049\n",
      "Epoch 47/100, Train Loss: 0.0010, Validation Loss: 0.0050\n",
      "Epoch 00047: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 48/100, Train Loss: 0.0010, Validation Loss: 0.0050\n",
      "Epoch 49/100, Train Loss: 0.0010, Validation Loss: 0.0051\n",
      "Epoch 50/100, Train Loss: 0.0009, Validation Loss: 0.0051\n",
      "Epoch 51/100, Train Loss: 0.0009, Validation Loss: 0.0051\n",
      "Epoch 52/100, Train Loss: 0.0009, Validation Loss: 0.0052\n",
      "Epoch 53/100, Train Loss: 0.0009, Validation Loss: 0.0052\n",
      "Epoch 54/100, Train Loss: 0.0009, Validation Loss: 0.0053\n",
      "Epoch 55/100, Train Loss: 0.0009, Validation Loss: 0.0053\n",
      "Epoch 56/100, Train Loss: 0.0009, Validation Loss: 0.0054\n",
      "Epoch 57/100, Train Loss: 0.0009, Validation Loss: 0.0054\n",
      "Epoch 58/100, Train Loss: 0.0009, Validation Loss: 0.0054\n",
      "Epoch 00058: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 59/100, Train Loss: 0.0009, Validation Loss: 0.0054\n",
      "Epoch 60/100, Train Loss: 0.0009, Validation Loss: 0.0055\n",
      "Epoch 61/100, Train Loss: 0.0009, Validation Loss: 0.0055\n",
      "Epoch 62/100, Train Loss: 0.0009, Validation Loss: 0.0056\n",
      "Epoch 63/100, Train Loss: 0.0009, Validation Loss: 0.0056\n",
      "Epoch 64/100, Train Loss: 0.0009, Validation Loss: 0.0056\n",
      "Epoch 65/100, Train Loss: 0.0009, Validation Loss: 0.0057\n",
      "Epoch 66/100, Train Loss: 0.0009, Validation Loss: 0.0057\n",
      "Epoch 67/100, Train Loss: 0.0009, Validation Loss: 0.0058\n",
      "Epoch 68/100, Train Loss: 0.0009, Validation Loss: 0.0058\n",
      "Epoch 69/100, Train Loss: 0.0008, Validation Loss: 0.0059\n",
      "Epoch 00069: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 70/100, Train Loss: 0.0008, Validation Loss: 0.0059\n",
      "Epoch 71/100, Train Loss: 0.0008, Validation Loss: 0.0059\n",
      "Epoch 72/100, Train Loss: 0.0008, Validation Loss: 0.0059\n",
      "Epoch 73/100, Train Loss: 0.0008, Validation Loss: 0.0060\n",
      "Epoch 74/100, Train Loss: 0.0008, Validation Loss: 0.0060\n",
      "Epoch 75/100, Train Loss: 0.0008, Validation Loss: 0.0060\n",
      "Epoch 76/100, Train Loss: 0.0008, Validation Loss: 0.0060\n",
      "Epoch 77/100, Train Loss: 0.0008, Validation Loss: 0.0061\n",
      "Epoch 78/100, Train Loss: 0.0008, Validation Loss: 0.0061\n",
      "Epoch 79/100, Train Loss: 0.0008, Validation Loss: 0.0062\n",
      "Epoch 80/100, Train Loss: 0.0008, Validation Loss: 0.0062\n",
      "Epoch 00080: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 81/100, Train Loss: 0.0008, Validation Loss: 0.0062\n",
      "Epoch 82/100, Train Loss: 0.0008, Validation Loss: 0.0063\n",
      "Epoch 83/100, Train Loss: 0.0008, Validation Loss: 0.0063\n",
      "Epoch 84/100, Train Loss: 0.0008, Validation Loss: 0.0063\n",
      "Epoch 85/100, Train Loss: 0.0008, Validation Loss: 0.0063\n",
      "Epoch 86/100, Train Loss: 0.0008, Validation Loss: 0.0064\n",
      "Epoch 87/100, Train Loss: 0.0008, Validation Loss: 0.0064\n",
      "Epoch 88/100, Train Loss: 0.0008, Validation Loss: 0.0064\n",
      "Epoch 89/100, Train Loss: 0.0008, Validation Loss: 0.0065\n",
      "Epoch 90/100, Train Loss: 0.0008, Validation Loss: 0.0065\n",
      "Epoch 91/100, Train Loss: 0.0008, Validation Loss: 0.0066\n",
      "Epoch 00091: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 92/100, Train Loss: 0.0007, Validation Loss: 0.0065\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0066\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0066\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0066\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0067\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0067\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0067\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0068\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0068\n",
      "Fold 6 -> RMSE: 0.0590, MAE: 0.0451, R²: 0.9106\n",
      "\n",
      "Training Fold 7/10...\n",
      "Epoch 1/100, Train Loss: 0.0166, Validation Loss: 0.0060\n",
      "loss: 0.30972735555110376 0.30972735555110376\n",
      "Epoch 2/100, Train Loss: 0.0045, Validation Loss: 0.0027\n",
      "loss: 0.14172508915271465 0.14172508915271465\n",
      "Epoch 3/100, Train Loss: 0.0029, Validation Loss: 0.0022\n",
      "loss: 0.11432849134780554 0.11432849134780554\n",
      "Epoch 4/100, Train Loss: 0.0024, Validation Loss: 0.0020\n",
      "loss: 0.10337729724471956 0.10337729724471956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 0.0022, Validation Loss: 0.0020\n",
      "loss: 0.10272549661405961 0.10272549661405961\n",
      "Epoch 6/100, Train Loss: 0.0020, Validation Loss: 0.0019\n",
      "loss: 0.09985296874725691 0.09985296874725691\n",
      "Epoch 7/100, Train Loss: 0.0019, Validation Loss: 0.0020\n",
      "Epoch 8/100, Train Loss: 0.0018, Validation Loss: 0.0020\n",
      "Epoch 9/100, Train Loss: 0.0017, Validation Loss: 0.0021\n",
      "Epoch 10/100, Train Loss: 0.0016, Validation Loss: 0.0021\n",
      "Epoch 11/100, Train Loss: 0.0016, Validation Loss: 0.0021\n",
      "Epoch 12/100, Train Loss: 0.0015, Validation Loss: 0.0022\n",
      "Epoch 13/100, Train Loss: 0.0015, Validation Loss: 0.0021\n",
      "Epoch 14/100, Train Loss: 0.0015, Validation Loss: 0.0022\n",
      "Epoch 15/100, Train Loss: 0.0014, Validation Loss: 0.0022\n",
      "Epoch 16/100, Train Loss: 0.0014, Validation Loss: 0.0023\n",
      "Epoch 17/100, Train Loss: 0.0014, Validation Loss: 0.0023\n",
      "Epoch 00017: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 18/100, Train Loss: 0.0013, Validation Loss: 0.0024\n",
      "Epoch 19/100, Train Loss: 0.0013, Validation Loss: 0.0024\n",
      "Epoch 20/100, Train Loss: 0.0013, Validation Loss: 0.0024\n",
      "Epoch 21/100, Train Loss: 0.0012, Validation Loss: 0.0024\n",
      "Epoch 22/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 23/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 24/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 25/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 26/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 27/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 28/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 00028: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 29/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 30/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 31/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 32/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 33/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 34/100, Train Loss: 0.0011, Validation Loss: 0.0028\n",
      "Epoch 35/100, Train Loss: 0.0011, Validation Loss: 0.0028\n",
      "Epoch 36/100, Train Loss: 0.0011, Validation Loss: 0.0029\n",
      "Epoch 37/100, Train Loss: 0.0011, Validation Loss: 0.0029\n",
      "Epoch 38/100, Train Loss: 0.0010, Validation Loss: 0.0028\n",
      "Epoch 39/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 00039: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 40/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 41/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 42/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 43/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 44/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 45/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 46/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 47/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 48/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 49/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 50/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 00050: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 51/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 52/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 53/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 54/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 55/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 56/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 57/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 58/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 59/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 60/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 61/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 00061: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 62/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 63/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 64/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 65/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 66/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 67/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 68/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 69/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 70/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 71/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 72/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 00072: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 73/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 74/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 75/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 76/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 77/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 78/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 79/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 80/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 81/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 82/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 83/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 00083: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 84/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 85/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 86/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 87/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 88/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 89/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 90/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 91/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 92/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 00094: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Fold 7 -> RMSE: 0.0438, MAE: 0.0334, R²: 0.9536\n",
      "\n",
      "Training Fold 8/10...\n",
      "Epoch 1/100, Train Loss: 0.0161, Validation Loss: 0.0060\n",
      "loss: 0.3101696042713229 0.3101696042713229\n",
      "Epoch 2/100, Train Loss: 0.0040, Validation Loss: 0.0029\n",
      "loss: 0.1514611881560768 0.1514611881560768\n",
      "Epoch 3/100, Train Loss: 0.0025, Validation Loss: 0.0024\n",
      "loss: 0.12529115465190444 0.12529115465190444\n",
      "Epoch 4/100, Train Loss: 0.0021, Validation Loss: 0.0024\n",
      "loss: 0.12413377366306122 0.12413377366306122\n",
      "Epoch 5/100, Train Loss: 0.0019, Validation Loss: 0.0025\n",
      "Epoch 6/100, Train Loss: 0.0018, Validation Loss: 0.0025\n",
      "Epoch 7/100, Train Loss: 0.0017, Validation Loss: 0.0025\n",
      "Epoch 8/100, Train Loss: 0.0016, Validation Loss: 0.0026\n",
      "Epoch 9/100, Train Loss: 0.0016, Validation Loss: 0.0026\n",
      "Epoch 10/100, Train Loss: 0.0015, Validation Loss: 0.0027\n",
      "Epoch 11/100, Train Loss: 0.0015, Validation Loss: 0.0027\n",
      "Epoch 12/100, Train Loss: 0.0014, Validation Loss: 0.0027\n",
      "Epoch 13/100, Train Loss: 0.0014, Validation Loss: 0.0028\n",
      "Epoch 14/100, Train Loss: 0.0014, Validation Loss: 0.0028\n",
      "Epoch 15/100, Train Loss: 0.0013, Validation Loss: 0.0028\n",
      "Epoch 00015: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 16/100, Train Loss: 0.0013, Validation Loss: 0.0027\n",
      "Epoch 17/100, Train Loss: 0.0013, Validation Loss: 0.0028\n",
      "Epoch 18/100, Train Loss: 0.0012, Validation Loss: 0.0027\n",
      "Epoch 19/100, Train Loss: 0.0012, Validation Loss: 0.0028\n",
      "Epoch 20/100, Train Loss: 0.0012, Validation Loss: 0.0028\n",
      "Epoch 21/100, Train Loss: 0.0012, Validation Loss: 0.0028\n",
      "Epoch 22/100, Train Loss: 0.0012, Validation Loss: 0.0028\n",
      "Epoch 23/100, Train Loss: 0.0012, Validation Loss: 0.0029\n",
      "Epoch 24/100, Train Loss: 0.0012, Validation Loss: 0.0029\n",
      "Epoch 25/100, Train Loss: 0.0012, Validation Loss: 0.0030\n",
      "Epoch 26/100, Train Loss: 0.0011, Validation Loss: 0.0030\n",
      "Epoch 00026: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 27/100, Train Loss: 0.0011, Validation Loss: 0.0029\n",
      "Epoch 28/100, Train Loss: 0.0011, Validation Loss: 0.0029\n",
      "Epoch 29/100, Train Loss: 0.0011, Validation Loss: 0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 0.0011, Validation Loss: 0.0030\n",
      "Epoch 31/100, Train Loss: 0.0011, Validation Loss: 0.0029\n",
      "Epoch 32/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 33/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 34/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 35/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 36/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 37/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 00037: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 38/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 39/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 40/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 41/100, Train Loss: 0.0010, Validation Loss: 0.0031\n",
      "Epoch 42/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 43/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 44/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 45/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 46/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 47/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 48/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 00048: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 49/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 50/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 51/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 52/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 53/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 54/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 55/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 56/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 57/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 58/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 59/100, Train Loss: 0.0009, Validation Loss: 0.0034\n",
      "Epoch 00059: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 60/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 61/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 62/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 63/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 64/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 65/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 66/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 67/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 68/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 69/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 70/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 00070: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 71/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 72/100, Train Loss: 0.0008, Validation Loss: 0.0036\n",
      "Epoch 73/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 74/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 75/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 76/100, Train Loss: 0.0008, Validation Loss: 0.0037\n",
      "Epoch 77/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 78/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 79/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 80/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 81/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 00081: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 82/100, Train Loss: 0.0007, Validation Loss: 0.0038\n",
      "Epoch 83/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 84/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 85/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 86/100, Train Loss: 0.0007, Validation Loss: 0.0039\n",
      "Epoch 87/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 88/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 89/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 90/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 91/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 92/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 00092: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0040\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0041\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0042\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0042\n",
      "Fold 8 -> RMSE: 0.0489, MAE: 0.0400, R²: 0.9199\n",
      "\n",
      "Training Fold 9/10...\n",
      "Epoch 1/100, Train Loss: 0.0143, Validation Loss: 0.0051\n",
      "loss: 0.26467849909806773 0.26467849909806773\n",
      "Epoch 2/100, Train Loss: 0.0039, Validation Loss: 0.0032\n",
      "loss: 0.16691697806822958 0.16691697806822958\n",
      "Epoch 3/100, Train Loss: 0.0024, Validation Loss: 0.0029\n",
      "loss: 0.15115998937588415 0.15115998937588415\n",
      "Epoch 4/100, Train Loss: 0.0021, Validation Loss: 0.0028\n",
      "loss: 0.14374328697977035 0.14374328697977035\n",
      "Epoch 5/100, Train Loss: 0.0019, Validation Loss: 0.0026\n",
      "loss: 0.1377608644963857 0.1377608644963857\n",
      "Epoch 6/100, Train Loss: 0.0017, Validation Loss: 0.0025\n",
      "loss: 0.12858113599577337 0.12858113599577337\n",
      "Epoch 7/100, Train Loss: 0.0016, Validation Loss: 0.0025\n",
      "Epoch 8/100, Train Loss: 0.0016, Validation Loss: 0.0025\n",
      "Epoch 9/100, Train Loss: 0.0015, Validation Loss: 0.0025\n",
      "Epoch 10/100, Train Loss: 0.0015, Validation Loss: 0.0025\n",
      "loss: 0.12813215379239296 0.12813215379239296\n",
      "Epoch 11/100, Train Loss: 0.0014, Validation Loss: 0.0025\n",
      "loss: 0.12758380342620512 0.12758380342620512\n",
      "Epoch 12/100, Train Loss: 0.0014, Validation Loss: 0.0024\n",
      "loss: 0.1257885438408266 0.1257885438408266\n",
      "Epoch 13/100, Train Loss: 0.0014, Validation Loss: 0.0025\n",
      "Epoch 14/100, Train Loss: 0.0013, Validation Loss: 0.0025\n",
      "Epoch 15/100, Train Loss: 0.0013, Validation Loss: 0.0025\n",
      "Epoch 16/100, Train Loss: 0.0013, Validation Loss: 0.0025\n",
      "Epoch 17/100, Train Loss: 0.0013, Validation Loss: 0.0025\n",
      "Epoch 18/100, Train Loss: 0.0013, Validation Loss: 0.0025\n",
      "Epoch 19/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 20/100, Train Loss: 0.0012, Validation Loss: 0.0025\n",
      "Epoch 21/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 22/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 23/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 00023: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 24/100, Train Loss: 0.0012, Validation Loss: 0.0026\n",
      "Epoch 25/100, Train Loss: 0.0011, Validation Loss: 0.0026\n",
      "Epoch 26/100, Train Loss: 0.0011, Validation Loss: 0.0026\n",
      "Epoch 27/100, Train Loss: 0.0011, Validation Loss: 0.0026\n",
      "Epoch 28/100, Train Loss: 0.0011, Validation Loss: 0.0026\n",
      "Epoch 29/100, Train Loss: 0.0011, Validation Loss: 0.0026\n",
      "Epoch 30/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 31/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 32/100, Train Loss: 0.0011, Validation Loss: 0.0028\n",
      "Epoch 33/100, Train Loss: 0.0011, Validation Loss: 0.0027\n",
      "Epoch 34/100, Train Loss: 0.0011, Validation Loss: 0.0028\n",
      "Epoch 00034: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 35/100, Train Loss: 0.0010, Validation Loss: 0.0027\n",
      "Epoch 36/100, Train Loss: 0.0010, Validation Loss: 0.0028\n",
      "Epoch 37/100, Train Loss: 0.0010, Validation Loss: 0.0028\n",
      "Epoch 38/100, Train Loss: 0.0010, Validation Loss: 0.0028\n",
      "Epoch 39/100, Train Loss: 0.0010, Validation Loss: 0.0028\n",
      "Epoch 40/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 41/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 42/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 43/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 44/100, Train Loss: 0.0010, Validation Loss: 0.0029\n",
      "Epoch 45/100, Train Loss: 0.0010, Validation Loss: 0.0030\n",
      "Epoch 00045: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 46/100, Train Loss: 0.0009, Validation Loss: 0.0030\n",
      "Epoch 47/100, Train Loss: 0.0009, Validation Loss: 0.0030\n",
      "Epoch 48/100, Train Loss: 0.0009, Validation Loss: 0.0030\n",
      "Epoch 49/100, Train Loss: 0.0009, Validation Loss: 0.0030\n",
      "Epoch 50/100, Train Loss: 0.0009, Validation Loss: 0.0030\n",
      "Epoch 51/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 52/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 53/100, Train Loss: 0.0009, Validation Loss: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 55/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 56/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 00056: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 57/100, Train Loss: 0.0009, Validation Loss: 0.0031\n",
      "Epoch 58/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 59/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 60/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 61/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 62/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 63/100, Train Loss: 0.0009, Validation Loss: 0.0032\n",
      "Epoch 64/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 65/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 66/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 67/100, Train Loss: 0.0009, Validation Loss: 0.0033\n",
      "Epoch 00067: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 68/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 69/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 70/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 71/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 72/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 73/100, Train Loss: 0.0008, Validation Loss: 0.0033\n",
      "Epoch 74/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 75/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 76/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 77/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 78/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 00078: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 79/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 80/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 81/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 82/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 83/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 84/100, Train Loss: 0.0008, Validation Loss: 0.0034\n",
      "Epoch 85/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 86/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 87/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 88/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 89/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 00089: reducing learning rate of group 0 to 4.7830e-04.\n",
      "Epoch 90/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 91/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 92/100, Train Loss: 0.0008, Validation Loss: 0.0035\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0035\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0036\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0037\n",
      "Epoch 00100: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Fold 9 -> RMSE: 0.0492, MAE: 0.0375, R²: 0.9321\n",
      "\n",
      "Training Fold 10/10...\n",
      "Epoch 1/100, Train Loss: 0.0152, Validation Loss: 0.0086\n",
      "loss: 0.4462453896267107 0.4462453896267107\n",
      "Epoch 2/100, Train Loss: 0.0041, Validation Loss: 0.0028\n",
      "loss: 0.14564191349427347 0.14564191349427347\n",
      "Epoch 3/100, Train Loss: 0.0021, Validation Loss: 0.0027\n",
      "loss: 0.13806433925219608 0.13806433925219608\n",
      "Epoch 4/100, Train Loss: 0.0017, Validation Loss: 0.0027\n",
      "Epoch 5/100, Train Loss: 0.0015, Validation Loss: 0.0027\n",
      "Epoch 6/100, Train Loss: 0.0014, Validation Loss: 0.0028\n",
      "Epoch 7/100, Train Loss: 0.0014, Validation Loss: 0.0028\n",
      "Epoch 8/100, Train Loss: 0.0013, Validation Loss: 0.0029\n",
      "Epoch 9/100, Train Loss: 0.0013, Validation Loss: 0.0029\n",
      "Epoch 10/100, Train Loss: 0.0012, Validation Loss: 0.0030\n",
      "Epoch 11/100, Train Loss: 0.0012, Validation Loss: 0.0030\n",
      "Epoch 12/100, Train Loss: 0.0012, Validation Loss: 0.0030\n",
      "Epoch 13/100, Train Loss: 0.0012, Validation Loss: 0.0031\n",
      "Epoch 14/100, Train Loss: 0.0012, Validation Loss: 0.0031\n",
      "Epoch 00014: reducing learning rate of group 0 to 9.0000e-04.\n",
      "Epoch 15/100, Train Loss: 0.0011, Validation Loss: 0.0031\n",
      "Epoch 16/100, Train Loss: 0.0011, Validation Loss: 0.0032\n",
      "Epoch 17/100, Train Loss: 0.0011, Validation Loss: 0.0032\n",
      "Epoch 18/100, Train Loss: 0.0011, Validation Loss: 0.0032\n",
      "Epoch 19/100, Train Loss: 0.0011, Validation Loss: 0.0032\n",
      "Epoch 20/100, Train Loss: 0.0010, Validation Loss: 0.0033\n",
      "Epoch 21/100, Train Loss: 0.0010, Validation Loss: 0.0033\n",
      "Epoch 22/100, Train Loss: 0.0010, Validation Loss: 0.0034\n",
      "Epoch 23/100, Train Loss: 0.0010, Validation Loss: 0.0034\n",
      "Epoch 24/100, Train Loss: 0.0010, Validation Loss: 0.0034\n",
      "Epoch 25/100, Train Loss: 0.0010, Validation Loss: 0.0034\n",
      "Epoch 00025: reducing learning rate of group 0 to 8.1000e-04.\n",
      "Epoch 26/100, Train Loss: 0.0010, Validation Loss: 0.0034\n",
      "Epoch 27/100, Train Loss: 0.0010, Validation Loss: 0.0035\n",
      "Epoch 28/100, Train Loss: 0.0009, Validation Loss: 0.0035\n",
      "Epoch 29/100, Train Loss: 0.0010, Validation Loss: 0.0035\n",
      "Epoch 30/100, Train Loss: 0.0009, Validation Loss: 0.0036\n",
      "Epoch 31/100, Train Loss: 0.0009, Validation Loss: 0.0036\n",
      "Epoch 32/100, Train Loss: 0.0009, Validation Loss: 0.0036\n",
      "Epoch 33/100, Train Loss: 0.0009, Validation Loss: 0.0036\n",
      "Epoch 34/100, Train Loss: 0.0009, Validation Loss: 0.0036\n",
      "Epoch 35/100, Train Loss: 0.0009, Validation Loss: 0.0037\n",
      "Epoch 36/100, Train Loss: 0.0009, Validation Loss: 0.0037\n",
      "Epoch 00036: reducing learning rate of group 0 to 7.2900e-04.\n",
      "Epoch 37/100, Train Loss: 0.0009, Validation Loss: 0.0037\n",
      "Epoch 38/100, Train Loss: 0.0009, Validation Loss: 0.0037\n",
      "Epoch 39/100, Train Loss: 0.0009, Validation Loss: 0.0038\n",
      "Epoch 40/100, Train Loss: 0.0009, Validation Loss: 0.0037\n",
      "Epoch 41/100, Train Loss: 0.0009, Validation Loss: 0.0038\n",
      "Epoch 42/100, Train Loss: 0.0009, Validation Loss: 0.0038\n",
      "Epoch 43/100, Train Loss: 0.0009, Validation Loss: 0.0038\n",
      "Epoch 44/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 45/100, Train Loss: 0.0008, Validation Loss: 0.0038\n",
      "Epoch 46/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 47/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 00047: reducing learning rate of group 0 to 6.5610e-04.\n",
      "Epoch 48/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 49/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 50/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 51/100, Train Loss: 0.0008, Validation Loss: 0.0039\n",
      "Epoch 52/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 53/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 54/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 55/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 56/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 57/100, Train Loss: 0.0008, Validation Loss: 0.0040\n",
      "Epoch 58/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 00058: reducing learning rate of group 0 to 5.9049e-04.\n",
      "Epoch 59/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 60/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 61/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 62/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 63/100, Train Loss: 0.0008, Validation Loss: 0.0041\n",
      "Epoch 64/100, Train Loss: 0.0008, Validation Loss: 0.0042\n",
      "Epoch 65/100, Train Loss: 0.0008, Validation Loss: 0.0042\n",
      "Epoch 66/100, Train Loss: 0.0008, Validation Loss: 0.0042\n",
      "Epoch 67/100, Train Loss: 0.0007, Validation Loss: 0.0042\n",
      "Epoch 68/100, Train Loss: 0.0007, Validation Loss: 0.0042\n",
      "Epoch 69/100, Train Loss: 0.0007, Validation Loss: 0.0043\n",
      "Epoch 00069: reducing learning rate of group 0 to 5.3144e-04.\n",
      "Epoch 70/100, Train Loss: 0.0007, Validation Loss: 0.0043\n",
      "Epoch 71/100, Train Loss: 0.0007, Validation Loss: 0.0043\n",
      "Epoch 72/100, Train Loss: 0.0007, Validation Loss: 0.0043\n",
      "Epoch 73/100, Train Loss: 0.0007, Validation Loss: 0.0043\n",
      "Epoch 74/100, Train Loss: 0.0007, Validation Loss: 0.0044\n",
      "Epoch 75/100, Train Loss: 0.0007, Validation Loss: 0.0044\n",
      "Epoch 76/100, Train Loss: 0.0007, Validation Loss: 0.0044\n",
      "Epoch 77/100, Train Loss: 0.0007, Validation Loss: 0.0044\n",
      "Epoch 78/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 79/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 80/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 00080: reducing learning rate of group 0 to 4.7830e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 82/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 83/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 84/100, Train Loss: 0.0007, Validation Loss: 0.0045\n",
      "Epoch 85/100, Train Loss: 0.0007, Validation Loss: 0.0046\n",
      "Epoch 86/100, Train Loss: 0.0007, Validation Loss: 0.0046\n",
      "Epoch 87/100, Train Loss: 0.0007, Validation Loss: 0.0046\n",
      "Epoch 88/100, Train Loss: 0.0007, Validation Loss: 0.0047\n",
      "Epoch 89/100, Train Loss: 0.0007, Validation Loss: 0.0047\n",
      "Epoch 90/100, Train Loss: 0.0007, Validation Loss: 0.0047\n",
      "Epoch 91/100, Train Loss: 0.0007, Validation Loss: 0.0047\n",
      "Epoch 00091: reducing learning rate of group 0 to 4.3047e-04.\n",
      "Epoch 92/100, Train Loss: 0.0007, Validation Loss: 0.0048\n",
      "Epoch 93/100, Train Loss: 0.0007, Validation Loss: 0.0048\n",
      "Epoch 94/100, Train Loss: 0.0007, Validation Loss: 0.0048\n",
      "Epoch 95/100, Train Loss: 0.0007, Validation Loss: 0.0048\n",
      "Epoch 96/100, Train Loss: 0.0007, Validation Loss: 0.0048\n",
      "Epoch 97/100, Train Loss: 0.0007, Validation Loss: 0.0049\n",
      "Epoch 98/100, Train Loss: 0.0007, Validation Loss: 0.0049\n",
      "Epoch 99/100, Train Loss: 0.0007, Validation Loss: 0.0049\n",
      "Epoch 100/100, Train Loss: 0.0007, Validation Loss: 0.0049\n",
      "Fold 10 -> RMSE: 0.0515, MAE: 0.0400, R²: 0.9475\n",
      "\n",
      "Cross-validation Results:\n",
      "RMSE Mean: 0.0753, Std: 0.0370\n",
      "MAE Mean: 0.0563, Std: 0.0265\n",
      "R² Mean: 0.8182, Std: 0.2215\n"
     ]
    }
   ],
   "source": [
    "# 进行5折交叉验证\n",
    "epoch = 100 #设置训练轮数\n",
    "mean_rmse, mean_mae, mean_r2, rmse_std, mae_std, r2_std = train_kfold(model, data_list, mol_features, train_targets, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60705d9e",
   "metadata": {},
   "source": [
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# 以下为测试部分\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49f0ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型测试函数\n",
    "def test_model(model, data_list, mol_features, targets):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    loss_fn = nn.MSELoss()\n",
    "    pred_list = []\n",
    "    targ_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data_list)):\n",
    "            data_ = data_list[i].to(device)\n",
    "#             mol_feature = torch.tensor(mol_features[i], dtype=torch.float32).to(device)\n",
    "#             target = torch.tensor(targets[i], dtype=torch.float32).to(device)\n",
    "            mol_feature = mol_features[i].clone().detach().to(device)\n",
    "            target = targets[i].clone().detach().to(device)\n",
    "            output = model(data_,mol_feature)\n",
    "            # print(output, target)\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss += loss.item()\n",
    "            pred_list.append(output.item())\n",
    "            targ_list.append(target.item())\n",
    "    rmse = np.sqrt(mean_squared_error(targ_list, pred_list))\n",
    "    mae = mean_absolute_error(targ_list, pred_list)\n",
    "    r2 = r2_score(targ_list, pred_list)\n",
    "    return val_loss / len(data_list), rmse, mae, r2, targ_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6c184701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集数据处理\n",
    "smiles_list_test = test_data['SMILES'].tolist()\n",
    "test_targets = test_data['IE'].values\n",
    "mol_features_test = test_data.drop(columns=['SMILES', 'IE']).values\n",
    "\n",
    "mol_features_test = torch.tensor(mol_features_test, dtype=torch.float32).to(device)\n",
    "test_targets = torch.tensor(test_targets, dtype=torch.float32).to(device)\n",
    "\n",
    "# 分子图特征提取\n",
    "featurization_params = FeaturizationParameters()\n",
    "data_list_test = []\n",
    "for smiles in smiles_list_test:\n",
    "    data, mol = smiles_to_graph(smiles, featurization_params)  # 传递 featurization_params\n",
    "    #print(data)\n",
    "    data_list_test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa26bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集MSE误差 0.0018120401735556397 \n",
      "训练集MAE误差 0.03105267282875105 \n",
      "训练集RMSE误差 0.04256806518454462 \n",
      "训练集R平方 0.9567650762817508 \n",
      "训练集平均损失 0.0018120401805653548 \n",
      "测试集MSE误差 0.02033678802170041 \n",
      "测试集MAE误差 0.09897951884124115 \n",
      "测试集RMSE误差 0.14260711069824117 \n",
      "测试集R平方 0.3717724738167685 \n",
      "测试集平均损失 0.020336787910965257\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "1.# 训练集\n",
    "train_val_loss, train_rmse, train_mae, train_r_square, train_target, train_pred = test_model(model, data_list, mol_features, train_targets)\n",
    "2.# 测试集\n",
    "test_val_loss, test_rmse, test_mae, test_r_square, test_target, test_pred = test_model(model, data_list_test, mol_features_test, test_targets)\n",
    "\n",
    "train_mse = train_rmse * train_rmse\n",
    "test_mse = test_rmse * test_rmse\n",
    "\n",
    "# 输出\n",
    "print('训练集MSE误差', train_mse, '\\n训练集MAE误差', train_mae,'\\n训练集RMSE误差', train_rmse, '\\n训练集R平方', train_r_square,\n",
    "      '\\n训练集平均损失', train_val_loss, '\\n测试集MSE误差', test_mse, '\\n测试集MAE误差', test_mae,'\\n测试集RMSE误差', test_rmse,\n",
    "      '\\n测试集R平方', test_r_square, '\\n测试集平均损失', test_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42bfeb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "plt.plot(train_target, train_target, color='yellow', linewidth=2)\n",
    "plt.scatter(train_target, train_pred,5,  color='red')\n",
    "#plt.scatter(train_target, y_pred_svm,5,  color='cyan')\n",
    "#plt.scatter(train_target, y_train_pred,10,  color='black')\n",
    "plt.title('缓蚀剂性能训练集')\n",
    "plt.xlabel('真实值')\n",
    "plt.ylabel('预测值')\n",
    "#plt.legend(('','rf','svm','gnn'))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cdbfdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "plt.plot(test_target, test_target, color='yellow', linewidth=2)\n",
    "plt.scatter(test_target, y_pred_rf,5,  color='red')\n",
    "plt.scatter(test_target, y_pred_svm,5,  color='cyan')\n",
    "plt.scatter(test_target, test_pred,10,  color='black')\n",
    "plt.title('缓蚀剂性能测试集')\n",
    "plt.xlabel('真实值')\n",
    "plt.ylabel('预测值')\n",
    "plt.legend(('','rf','svm','gnn'))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d562f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ecfp(mol, rad=2, bits=1024):\n",
    "    if mol is None:\n",
    "        return None\n",
    "    morgan_gen = Chem.rdFingerprintGenerator.GetMorganGenerator(radius=rad, fpSize=bits)\n",
    "    fingerprint = morgan_gen.GetFingerprint(mol)\n",
    "    array = np.zeros(len(fingerprint))\n",
    "    DataStructs.ConvertToNumpyArray(fingerprint, array)\n",
    "    # array = Chem.rdFingerprintDescriptors.GetMorganFingerprint(mol, rad)\n",
    "    return array\n",
    "\n",
    "def readMol(df):\n",
    "    df['mol'] = df['SMILES'].apply(Chem.MolFromSmiles)\n",
    "    df['ecfp'] = df['mol'].apply(generate_ecfp)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "745569a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = readMol(train_data)\n",
    "df_test = readMol(test_data)\n",
    "X_train = df_train['ecfp'].tolist()\n",
    "X_test = df_test['ecfp'].tolist()\n",
    "y_train = df_train['IE']\n",
    "y_test = df_test['IE']\n",
    "train_data = train_data.drop('ecfp', axis=1).drop('mol', axis=1)\n",
    "test_data = test_data.drop('ecfp', axis=1).drop('mol', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8c634bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM MSE: 0.0257\n",
      "SVM R2: 0.2065\n",
      "SVM RMSE: 0.1603\n",
      "SVM MAE: 0.1254\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svm = SVR()\n",
    "# 使用 GridSearchCV 进行超参数调优\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 获取最佳模型\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# 预测\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "mse_svm = mean_squared_error(y_test, y_pred_svm)\n",
    "rmse_svm  = np.sqrt(mse_svm)\n",
    "mae_svm = mean_absolute_error(y_test, y_pred_svm)\n",
    "r_square_svm = r2_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f'SVM MSE: {mse_svm:.4f}')\n",
    "print(f'SVM R2: {r_square_svm:.4f}')\n",
    "print(f'SVM RMSE: {rmse_svm:.4f}')\n",
    "print(f'SVM MAE: {mae_svm:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dff9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF MSE: 0.0279\n",
      "RF R2: 0.1385\n",
      "RF RMSE: 0.1670\n",
      "RF MAE: 0.1238\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# 评估模型\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf  = np.sqrt(mse_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r_square_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f'RF MSE: {mse_rf:.4f}')\n",
    "print(f'RF R2: {r_square_rf:.4f}')\n",
    "print(f'RF RMSE: {rmse_rf:.4f}')\n",
    "print(f'RF MAE: {mae_rf:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
