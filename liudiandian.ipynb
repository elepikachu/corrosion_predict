{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9005bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem, Draw, Lipinski, Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b5e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac995e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Atom' object has no attribute 'GetAtomsNum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 将原子特征转换为(num_atoms, 148)形状\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atom \u001b[38;5;129;01min\u001b[39;00m mol\u001b[38;5;241m.\u001b[39mGetAtoms():\n\u001b[0;32m     30\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 31\u001b[0m         \u001b[43matom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetAtomsNum\u001b[49m(),\n\u001b[0;32m     32\u001b[0m         atom\u001b[38;5;241m.\u001b[39mGetDegree(),\n\u001b[0;32m     33\u001b[0m         atom\u001b[38;5;241m.\u001b[39mGetTotalNumHs\n\u001b[0;32m     34\u001b[0m     ]\n\u001b[0;32m     35\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(features, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#强制转换为float32\u001b[39;00m\n\u001b[0;32m     36\u001b[0m atom_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mrepeat(num_atoms, \u001b[38;5;241m148\u001b[39m) \u001b[38;5;66;03m# (num_atoms, 148)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Atom' object has no attribute 'GetAtomsNum'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb14ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f438f2ef",
   "metadata": {},
   "source": [
    "从CSV文件中加载分子数据，并将这些数据转换为图（graph）表示，以便于后续使用图神经网络（GNNs）进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e8d87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file):\n",
    "    data = pd.read_csv(csv_file, encoding='ANSI')\n",
    "    molecules = []\n",
    "    max_atoms = 0 #最大原子数\n",
    "    max_edges = 0 # 最大边数\n",
    "    #按照id分组处理\n",
    "    for  _,group in data.groupby('ID'):\n",
    "        for _,row in group.iterrows():\n",
    "            IE = row['IE']\n",
    "            smiles = row['SMILES']\n",
    "            molecule_id = row['ID']\n",
    "          #  print(smiles)\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            # SMILE 无效 → 跳过\n",
    "            if mol is None:\n",
    "                continue\n",
    "            num_atoms = len(mol.GetAtoms()) # 原子数获取\n",
    "            max_atoms = max(max_atoms, num_atoms) # 最大原子数更新\n",
    "            bonds = mol.GetBonds()\n",
    "            num_edges = len(bonds) #边数获取\n",
    "#             adj_matrix = np.zeros((num_atoms, num_atoms)) # 邻接矩阵初始化\n",
    "#             # 填充邻接矩阵，建立连接关系(无向图)\n",
    "#             for bond in bonds:\n",
    "#                 i = bond.GetBeginAtomIdx()\n",
    "#                 j = bond.GetEndAtomIdx()\n",
    "#                 adj_matrix[i,j] = 1\n",
    "#                 adj_matrix[j,i] = 1 #无向图\n",
    "            adj_matrix = Chem.GetAdjacencyMatrix(mol)  # 使用RDKit直接获取邻接矩阵\n",
    "            # 将原子特征转换为(num_atoms, 148)形状\n",
    "            for atom in mol.GetAtoms():\n",
    "                features = [\n",
    "                    atom.GetAtomsNum(),\n",
    "                    atom.GetDegree(),\n",
    "                    atom.GetTotalNumHs\n",
    "                ]\n",
    "            features = np.array(features, dtype=np.float32) #强制转换为float32\n",
    "            atom_features = torch.tensor(features, dtype=torch.float).repeat(num_atoms, 148) # (num_atoms, 148)\n",
    "            # 填充atom_features 到 max_atom\n",
    "            if num_atoms < max_atoms:\n",
    "                padding = torch.zeros((max_atoms-num_atoms, 148), dtype=torch.float)\n",
    "                atom_features = torch.cat([atom_features, padding], dim=0)\n",
    "            # 处理邻接矩阵，转化为edge_index\n",
    "            edge_index = torch.tensor(np.array(np.nonzero(adj_matrix)), dtype=torch.long) # (2, num_edges)\n",
    "            # 更新最大边数的计算，基于实际边数\n",
    "            max_edges = max(max_edges, edge_index.shape[1])\n",
    "#             # 如果edge_index边数小于max_edges,需要填充到max_edges\n",
    "#             if edge_index.shape[i] < max_edges:\n",
    "#                 padding_edges = torch.zeros((2, int(max_edges - edge_index.shipe[i])), dtype=torch.long)\n",
    "#                 edge_index = torch.cat([edge_index, padding_edges], dim=1)\n",
    "\n",
    "            #print(f\"edge_index.shape:{edge_index.shape}\")\n",
    "            #print(f\"atom_feature.shape:{atom_features.shape}\")\n",
    "\n",
    "            #保存分子信息\n",
    "            molecules.append ({\n",
    "                'x':atom_features,\n",
    "                'edge_index': edge_index,\n",
    "                'smiles': smiles,\n",
    "                'id': molecule_id,\n",
    "            })\n",
    "                \n",
    "    print(f\"最大原子数，{max_atoms}， 最大边数，{max_edges}\")\n",
    "    print(type(data))\n",
    "                \n",
    "    return molecules\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545dd82",
   "metadata": {},
   "source": [
    "加载额外的数据，并对这些数据进行标准化处理，同时保留ID列以供后续匹配或参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a2d4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_extra_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    id_list = df['ID'].values\n",
    "    extra_data = df.drop(columns=['ID']).values # 删除id列，提取特征数据\n",
    "    scaler = StandardScaler()\n",
    "    extra_data_scale = scaler.fit_transform(extra_data)\n",
    "    extra_data_with_id = pd.DataFrame(extra_data_scale, columns=df.columns[1:])\n",
    "    extra_data_with_id.insert(0, 'ID', id_list) # 将id插入到第一列位置\n",
    "    return id_list, extra_data_with_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b17ed",
   "metadata": {},
   "source": [
    "图神经网络定义与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8327c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, dropout=0.3, num_layers=2):\n",
    "        super().__init__()\n",
    "        #定义图神经网络编码器层\n",
    "        self.encoder = nn.ModuleList([\n",
    "            GATConv(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        #定义解码器层\n",
    "        self.decoder = GATConv(hidden_dim, output_dim)\n",
    "        #Dropout层\n",
    "        self.dropout = dropout\n",
    "        #归一化层\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        #初始化网络参数\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        # 对图卷积层权重初始化\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, GATConv):\n",
    "                self.initialize_gatconv_weights(layer)\n",
    "\n",
    "        # 解码器层初始化\n",
    "        if isinstance(self.decoder, GATConv):\n",
    "            self.initialize_gatconv_weights(self.decoder)\n",
    "            \n",
    "    def initialize_gatconv_weights(self, gat_layer):\n",
    "        # 确保 gat_layer 不是 None\n",
    "        if gat_layer.lin_src is not None and gat_layer.lin_dst is not None:\n",
    "            nn.init.xavier_uniform_(gat_layer.lin_src.weight)\n",
    "            nn.init.xavier_uniform_(gat_layer.lin_dst.weight)\n",
    "            if gat_layer.attn_vecs is not None:\n",
    "                nn.init.xavier_uniform_(gat_layer.attn_vecs)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index = batch.x, batch.edge_index\n",
    "        edge_attr = batch.edge_attr if 'edge_attr' in batch else None\n",
    "\n",
    "        #前向传播过程，编码器+激活函数+层归一化\n",
    "        for layer in self.encoder:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = layer(x, edge_index, edge_attr=edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = self.layer_norm(x) #归一化\n",
    "\n",
    "        #解码层输出\n",
    "        output = self.decoder(x, edge_index, edge_attr=edge_attr)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b118e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练图神经网络模型\n",
    "def train_graph_nn(model, data_loader, optimizer, clip_val=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    num_samples = 0 # 添加这一行以修复未定义的变量\n",
    "    for data in data_loader:\n",
    "        print(data)\n",
    "        optimizer.zero_grad() #梯度清空\n",
    "        \n",
    "        output = model(data)#传递数据到前向计算\n",
    "        y = data.y\n",
    "        loss = loss_fun(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(y)\n",
    "        num_samples += len(y)\n",
    "    \n",
    "    avg_loss = total_loss / num_samples if num_samples > 0 else float('nan')\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db2396",
   "metadata": {},
   "source": [
    "数据批量合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf4a41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    print(batch)\n",
    "    # 获取所有图的原子特征x和边索引edgeindex\n",
    "    x_list = [data['x'] for data in batch] #每个data是torch_geometric.data.Data对象\n",
    "    edge_index_list = [data['edge_index'] for data in batch]\n",
    " #   smile_list = [data['smiles'] for data in batch]\n",
    " #   id_list = [data['id'] for data in batch]\n",
    "    \n",
    "    #拼接原子特征，确保图特征维度一致\n",
    "    x = torch.cat(x_list, dim=0)\n",
    "    print(f\"x shape after concatenation:{x.shape}\")\n",
    "    #拼接所有边索引，注意我们要对每个图边索引进行偏移，确保他们不会冲突\n",
    "    edge_index = []\n",
    "    num_nodes_accum = 0 # 累计节点数\n",
    "    for edge_idx in edge_index_list:\n",
    "        edge_idx_offset = edge_idx + num_nodes_accum\n",
    "        edge_index.append(edge_idx_offset)\n",
    "        num_nodes_accum += edge_idx.max().item() + 1\n",
    "        \n",
    "    #拼接所有边索引，确保形状是(2, num_edges)\n",
    "    edge_index = torch.cat(edge_index, dim=1)\n",
    "    print(f\"edge_index shape after concatenation:{edge_index.shape}\")\n",
    "        \n",
    "    #创建一个批量数据对象，确保返回batch对象\n",
    "    #batch_data = Batch(x=x, edge_index=edge_index, smiles=smile_list, id=id_list)\n",
    "    batch_data = Batch(x=x, edge_index=edge_index, y=y)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a657ea",
   "metadata": {},
   "source": [
    "功能预测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c68eb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fun_Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate=0.5):\n",
    "        super(Fun_Predictor, self).__init__()\n",
    "        \n",
    "        # 定义各层\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, 1)  # 假设最终预测是一个值\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, combined_features):\n",
    "            x = torch.relu(self.fc1(combined_features))\n",
    "            x = self.bn1(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.bn2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(self.fc3(x))\n",
    "            predict_IE = self.fc4(x)\n",
    "            return predict_IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecec7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fun_predictor(model_fun, graph_nn, molecule_data, extra_data_with_id, criterion):\n",
    "    model_fun.train()\n",
    "    graph_nn.eval()\n",
    "    total_loss = 0\n",
    "    graph_embedding = []\n",
    "    target_vals = []\n",
    "    data_list = []\n",
    "    \n",
    "    # 创建一个字典，将额外特征与id匹配，去除id列\n",
    "    extra_data_dict = {row['ID']:torch.tensor(row[1:],values,dtype=torch.float32) for _,row in extra_data_with_id.iterrows()}\n",
    "    \n",
    "    # 初始化一个列表保存额外特征值\n",
    "    extra_feature = []\n",
    "    \n",
    "    # 遍历所有额外特征数据，并为每个分子获取对应的图嵌入和特征\n",
    "    for _,row in extra_data_with_id.iterrows():\n",
    "        id_ = row['ID']\n",
    "        extra_feature_tensor = extra_data_dict[id_].unsqueeze(0)\n",
    "        molecule_data_for_id = next((data for data in molecule_data if data['id'] == id_), None)\n",
    "        if molecule_data_for_id is None:\n",
    "            raise ValueError(f\"No mol data found for ID:{id_}\")\n",
    "        x = molecule_data_for_id['x'] # 原子特征\n",
    "        edge_index = molecule_data_for_id['edge_index'] # 邻接矩阵\n",
    "        graph_data = Data(x=x, edge_index = edge_index)\n",
    "        \n",
    "        # 获取图嵌入\n",
    "        with torch.no_grad():\n",
    "            embedding = graph_nn(graph_data) #使用graphNN获取图嵌入\n",
    "            pooled_embedding = torch.mean(embedding, dim=0).unsqueeze(0) #平均池化\n",
    "            \n",
    "        # 合并图嵌入和额外特征\n",
    "        combined_embedding = torch.cat((pooled_embedding, extra_feature_tensor), dim=1)\n",
    "        graph_embeddings.append(combined_embedding)\n",
    "        \n",
    "        # 设置目标值，假设目标值是额外特征的最后一列\n",
    "        target_val = extra_feature_tensor[:, -1]  # 取最后一列作为目标值\n",
    "        target_vals.append(target_val)\n",
    "        \n",
    "    # 将所有图嵌入和目标值组合成批次\n",
    "    batch_embeddings = torch.cat(graph_embeddings, dim=0)\n",
    "    batch_targets = torch.cat(target_vals, dim=0).unsqueeze(-1)  # 确保目标值形状正确\n",
    "\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    predictions = model_fun(batch_embeddings)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(predictions, batch_targets)\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新模型参数\n",
    "    optimizer.step()\n",
    "\n",
    "    # 累积损失\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6cf0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主训练函数\n",
    "def main():\n",
    "    # 加载数据\n",
    "    csv_file_molecules = 'molecules.csv'  # 替换为您的分子数据文件路径\n",
    "    csv_file_extra = 'extra_data.csv'  # 替换为您的额外特征数据文件路径\n",
    "    \n",
    "    molecule_data = load_data(csv_file_molecules)\n",
    "    id_list, extra_data_with_id = load_extra_data(csv_file_extra)\n",
    "    \n",
    "    # 创建 Data 对象列表\n",
    "    data_list = [Data(x=data['x'], edge_index=data['edge_index'],  y=data['y']) for data in molecule_data]  # y 是占位符\n",
    "    \n",
    "    # 创建 DataLoader\n",
    "    data_loader = DataLoader(data_list, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # 定义模型参数\n",
    "    input_dim = 148  # 根据您的原子特征数量调整\n",
    "    output_dim = 64  # 图嵌入的维度\n",
    "    hidden_size = 128  # 全连接层的隐藏单元数量\n",
    "    \n",
    "    # 实例化模型\n",
    "    graph_nn = GraphNN(input_dim=input_dim, output_dim=output_dim)\n",
    "    fun_predictor = Fun_Predictor(input_size=output_dim + extra_data_with_id.shape[1] - 1, hidden_size=hidden_size)\n",
    "    \n",
    "    # 定义优化器和损失函数\n",
    "    optimizer_gnn = torch.optim.Adam(graph_nn.parameters(), lr=0.001)\n",
    "    optimizer_fp = torch.optim.Adam(fun_predictor.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练图神经网络\n",
    "        loss_gnn = train_graph_nn(graph_nn, data_loader, optimizer_gnn)\n",
    "        \n",
    "        # 训练功能预测模型\n",
    "        loss_fp = train_fun_predictor(fun_predictor, graph_nn, molecule_data, extra_data_with_id, criterion, optimizer_fp)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss_GNN: {loss_gnn:.4f}, Loss_FP: {loss_fp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e33db240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大原子数，158， 最大边数，334\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[Data(x=[158, 148], edge_index=[2, 24], y=[1]), Data(x=[158, 148], edge_index=[2, 18], y=[1]), Data(x=[158, 148], edge_index=[2, 56], y=[1]), Data(x=[158, 148], edge_index=[2, 56], y=[1]), Data(x=[95, 148], edge_index=[2, 24], y=[1]), Data(x=[158, 148], edge_index=[2, 44], y=[1]), Data(x=[158, 148], edge_index=[2, 18], y=[1]), Data(x=[158, 148], edge_index=[2, 72], y=[1]), Data(x=[158, 148], edge_index=[2, 50], y=[1]), Data(x=[158, 148], edge_index=[2, 80], y=[1]), Data(x=[158, 148], edge_index=[2, 72], y=[1]), Data(x=[158, 148], edge_index=[2, 22], y=[1]), Data(x=[158, 148], edge_index=[2, 66], y=[1]), Data(x=[158, 148], edge_index=[2, 50], y=[1]), Data(x=[158, 148], edge_index=[2, 60], y=[1]), Data(x=[158, 148], edge_index=[2, 56], y=[1]), Data(x=[158, 148], edge_index=[2, 100], y=[1]), Data(x=[95, 148], edge_index=[2, 40], y=[1]), Data(x=[158, 148], edge_index=[2, 70], y=[1]), Data(x=[158, 148], edge_index=[2, 22], y=[1]), Data(x=[158, 148], edge_index=[2, 30], y=[1]), Data(x=[158, 148], edge_index=[2, 56], y=[1]), Data(x=[95, 148], edge_index=[2, 86], y=[1]), Data(x=[158, 148], edge_index=[2, 50], y=[1]), Data(x=[158, 148], edge_index=[2, 50], y=[1]), Data(x=[158, 148], edge_index=[2, 18], y=[1]), Data(x=[95, 148], edge_index=[2, 74], y=[1]), Data(x=[95, 148], edge_index=[2, 50], y=[1]), Data(x=[158, 148], edge_index=[2, 266], y=[1]), Data(x=[158, 148], edge_index=[2, 30], y=[1]), Data(x=[158, 148], edge_index=[2, 54], y=[1]), Data(x=[158, 148], edge_index=[2, 112], y=[1])]\n",
      "x shape after concatenation:torch.Size([4741, 148])\n",
      "edge_index shape after concatenation:torch.Size([2, 1876])\n",
      "DataBatch(x=[4741, 148], edge_index=[2, 1876])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[59], line 34\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# 训练图神经网络\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     loss_gnn \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_graph_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_gnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# 训练功能预测模型\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     loss_fp \u001b[38;5;241m=\u001b[39m train_fun_predictor(fun_predictor, graph_nn, molecule_data, extra_data_with_id, criterion, optimizer_fp)\n",
      "Cell \u001b[1;32mIn[61], line 13\u001b[0m, in \u001b[0;36mtrain_graph_nn\u001b[1;34m(model, data_loader, optimizer, clip_val)\u001b[0m\n\u001b[0;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\u001b[38;5;66;03m#传递数据到前向计算\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   3316\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[0;32m   3317\u001b[0m     )\n\u001b[1;32m-> 3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3323\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   3324\u001b[0m     )\n\u001b[0;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada7323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0a5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e490cd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "y=torch.tensor(x, dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60732db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[149], edge_index=[2], edge_attr=[1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "data = Data(\n",
    "        x=torch.tensor(x, dtype=torch.float32),  # 确保x是浮点型\n",
    "        edge_index=[2, 7],\n",
    "        edge_attr=[7]\n",
    "    )\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd12ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60040725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个简单的神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)  # 第一个全连接层\n",
    "        self.fc2 = nn.Linear(20, 10)  # 第二个全连接层\n",
    "        self.fc3 = nn.Linear(10, 5)   # 第三个全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))      # 第一个层的输出\n",
    "        x = F.relu(self.fc2(x))      # 第二个层的输出\n",
    "        x = self.fc3(x)               # 第三个层的输出\n",
    "        return x\n",
    "    \n",
    "# 定义一个字典来存储每层的输出\n",
    "layer_outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5677069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    print(f\"Module {module.__class__.__name__}, Output: {output.shape},{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2765378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03c3429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x20d26972520>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9ba1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module Linear, Output: torch.Size([1, 20]),tensor([[-0.4132,  0.2326, -0.0092,  0.3946, -0.0758,  0.4080,  0.6119, -0.0347,\n",
      "         -0.1525, -0.3806, -0.1595, -0.0312, -0.5194,  0.1016,  0.3431,  0.2327,\n",
      "         -0.1706,  0.2782,  0.3920, -0.3863]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 准备一个输入张量\n",
    "input_tensor = torch.randn(1, 10)  # 随机生成一维张量，形状为(1, 10)\n",
    "\n",
    "# 进行前向传播\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148d466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0817ca79",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
